\renewcommand*{\thefootnote}{\arabic{footnote}}

\mbox{}\\
\vspace{8cm}

\section{Burden of bacterial pathogens}

Science has made great strides in understanding the ubiquity and complexity of microorganisms since the first observations by Antonie van Leeuwenhoek in the 17th century. The inquisitiveness of the scientific method revealed a myriad of microscopic shapes and interactions that play a pivotal role in global ecosystems and impact nearly every aspect of human activity, from beneficial applications in the food production industry to the detrimental effects on human and animal health. The latter association was discovered by Robert Koch in the 19th century, whose experiments enabled to identify bacteria as causative agents of deadly infectious diseases. Kochâ€™s contributions, along with those of other prominent scientists such as Louis Paster, established the fields of microbiology and bacteriology. The extensive research that followed revealed the enormous diversity of bacterial species, which today constitute one of the three domains of life established by Carl Woese in 1990.
Bacteria are single-celled organisms that lack a nuclear membrane and divide by binary fission. Superficially, bacteria may appear as simple forms of life. In reality, life is rarely simple and bacterial species exhibit tremendous diversity. This diversity arises from the selective forces that shape bacterial evolution towards the path that maximizes competitiveness in each habitat, giving rise to varied morphologies (e.g., cocci, rods, spirilla, filamentous), sizes (from as small as about 0.2 micrometer ($\mu m$) in diameter to more than 700 $\mu m$ in diameter), nutrient preferences () and structures (e.g. differences in cell wall structure between Gram-positive and Gram-negative bacteria).
Small structural differences can cause significant changes in the way bacteria interact with the environment or a host. For example, modifications in penicillin-binding proteins present in the cell wall, to which the antibiotic penicillin binds, consequently weakening the cell wall and leading to cell lysis, can confer reduced susceptibility or resistance to penicillin. The mechanisms through which bacteria develop \ac{AMR} are of great concern to public health. In 2021, 4.71 million deaths were estimated to be associated with bacterial \ac{AMR} \cite{naghavi_global_2024}. Although deaths attributed to \ac{AMR} among children under 5 years of age have been decreasing, deaths for older people have increased, and it is estimated that by 2050 8.22 million deaths may be associated with \ac{AMR}. Reducing the number of deaths related to \ac{AMR} is an important step to reduce the total number of deaths related to infection. A recent study estimated that in 2019 nearly 14 million deaths were infection-related, with 56\% of those deaths associated with only 33 bacterial pathogens.

(Figure 1.1: Adapted from \cite{ikuta_global_2022} (get data and show only the number of deaths per pathogen, highlight pathogens included in the \ac{BPPL}))

In the context of global mortality, these estimates put deaths associated with bacterial infections as responsible for at least 14\% of all global deaths \cite{ikuta_global_2022}. Moreover, the top five pathogens were associated with half of all bacterial deaths. These five pathogens are included in the \ac{WHO} \ac{BPPL} updated in 2024 \cite{noauthor_who_2024}. The \ac{BPPL} groups 15 families of antibiotic resistant pathogens into priority levels, with the aim of serving as a compass for \ac{RD} and for public health action. A global concerted effort of public health authorities is of the utmost importance to implement regionally tailored strategies to reduce the burden caused by bacterial infections. The implemented measures should combine infection prevention, vaccination, adequate methods of bacterial characterization and antibiotic usage, as well as promote \ac{RD} aiming to strengthen the resilience of these measures and future preparedness. Although all of these measures should be taken into consideration to develop effective strategies, the next sections will focus on the techniques used to characterize bacterial pathogens, as that will steer us towards the objectives of this dissertation.

\section{Bacterial characterization}

The characterization of pathogen samples is essential for the effective management of infected patients and the epidemiology of infectious diseases. Bacterial characterization or typing methodologies can be divided into phenotyping and genotyping \cite{li_bacterial_2009}. The former characterizes bacteria based on phenotypic assays, such as colony morphology on various culture media, biochemical tests, serology, and antibiotic susceptibility. The latter distinguishes bacteria based on their genetic content and has been increasingly adopted because it can provide results equivalent to or superior to phenotyping. Microbiologists use both approaches to infer specific phenotypic characteristics, such as susceptibility to antimicrobial drugs, allowing to set the best course of action for patient treatment or mitigate the impact of an outbreak.
Bacterial characterization relies on the expertise of clinical microbiologists who apply specialized and often species-specific techniques developed over decades. These techniques involve complex and multi-step protocols that depending on the bacterial pathogen can take days (rapid-growing bacteria such as pneumo?), to months (slow-growing bacteria such as Mycobacterium tuberculosis) \cite{didelot_transforming_2012}. The application of modern sequencing technologies, in combination with improved analytical methods, has allowed clinical microbiologists to reduce sample turnaround times by avoiding specific methodologies in favor of \ac{WGS}. Moreover, current analytical methods can accurately identify the genomic features associated with the phenotypic characteristics of interest from sequence data to provide equivalent results to more laborious and time-consuming lab protocols. Notwithstanding the impact of the latest developments in sequencing technologies and bioinformatics methods, there is still no single method for bacterial characterization that is universally ideal, with each method having to strike a balance between several desired characteristics, such as being applicable to all isolates, highly discriminatory at all levels, generating reproducible results at intra- and inter-laboratory level, while also using modest resources.
In the following sections, I present a brief overview of some of the most relevant methods used in the past and present of bacterial characterization, starting with classical phenotypic and molecular methods, and culminating in the latest sequencing technologies and bioinformatics methods used for \ac{WGS} and analysis of bacterial pathogens, which serve as the basis for the work presented in this dissertation.

(Figure 1.2: Adapted from ... sample processing workflow, simplified)

\section{Phenotypic methods}

Classical bacteriology methodologies are based on successfully isolating a bacterial pathogen through culturing. Given that different bacterial species may have different growth requirements, microbiologists had to develop a wide repertoire of techniques to account for all the variable requirements. After successfully isolating a pathogen, microbiologists will perform a series of tests to determine the species of the pathogen and its antimicrobial and virulence profiles. In surveillance and outbreak investigation settings, intra-species typing may be performed to identify features that allow for a more accurate analysis of strain similarity, which is especially important to identify circulating lineages and potential chains of transmission.

The culture step varies according to the complexity of a sample. For samples from usually sterile sites, such as cerebrospinal fluid, it may be possible to report all organisms present in the sample and it is simpler to identify the ones that are clinically relevant and should go through further analysis steps. In the case of complex samples, such as faeces, isolating the \textit{micro culprit} may require a more custom approach guided by an educated guess about likely pathogens to select the appropriate media for culture and subsequent tests for a definitive diagnostic.

A correct species identification is highly informative, as it allows to deduce intrinsic characteristics from the body of knowledge and estimate the pathogenic potential, especially in the context of the isolation site. To identify the species of an isolate, microbiologists may use Gram staining, evaluate colony growth and morphology, and perform rapid biochemical tests. Determining the biomolecule profiles of pure suspensions through \ac{MALDI-TOF} mass spectrometry and comparing them with known profiles is also used for rapid species identification.

Following culture and species identification, the determination of the antimicrobial resistance profile is crucial to select an effective treatment for infected patients. Antimicrobial resistance tests are mainly based on inhibition of \textit{in vitro} bacterial growth when exposed to an antibiotic. The efficacy of testing methods, such as disc diffusion and E-TEST, is compared against gold-standard susceptibility-testing systems, such as micro-dilution, to infer \textit{in vivo} efficacy.
The level of susceptibility to a given antibiotic is based on the \ac{MIC} and on the definition of \textit{breakpoints}, which correspond to the antibiotic concentration above which an isolate is considered to be resistant to therapy \cite{didelot_transforming_2012}. \textit{Breakpoints} are defined based on various factors that are not necessarily universally agreed upon, making it difficult to accurately compare and assess the efficacy of susceptibility testing and associate it with clinical outcome. Moreover, it is important to note that the results of susceptibility testing may not translate into similar \textit{in vivo} results, as resistance mechanisms may be more complex and depend on factors not adequately emulated by current susceptibility testing practices \cite{didelot_transforming_2012, hassall_limitations_2024}.

Compared to antimicrobial susceptibility testing, the detection of virulence factors tends to be overlooked when selecting an effective treatment for patients. Nonetheless, knowledge of the virulence profile of pathogens can play an important role when the presence of a virulence factor is known to contribute significantly to pathogenesis and disease severity. For example, toxin-producing strains of \textit{Clostridioides difficile} are more pathogenic and may require differential treatment. In public health, virulence factors are especially important as vaccine targets, such as the capsule polysaccharide of \textit{Streptococcus pneumoniae}, whose variability is detected by serotyping, and the most relevant serotypes are targeted for vaccine development \cite{henrichsen_six_1995, tarrago_identification_2008, silva-costa_adult_2023, musher_remarkable_2022}.

\section{Genotypic methods}

With the introduction of molecular methods for bacterial characterization, the basis for systematics changed. The distinction based on classical phenotypic criteria was in part replaced by molecular criteria, particularly molecular sequences, since these methods can offer greater granularity, resulting in more precise phylogenetic analyses and diagnostics. Molecular methods can be divided into three main categories: i) \ac{DNA} banding pattern-; ii) \ac{DNA} hybridization-; and \ac{DNA} sequencing-based methods. The first differentiates bacterial strains based on the size and pattern of \ac{DNA} bands/fragments generated by amplification of genomic \ac{DNA} or by cleavage of \ac{DNA} using \ac{REs}. The second uses techniques such as \ac{DNA} macroarrays and microarrays, which distinguish strains through hybridization to probes complementary to known sequences. The third determine and compare the \ac{DNA} sequence of genomic regions of interest, often determinant for a particular feature, to discriminate bacterial strains based on sequence variation.

\subsection{DNA banding pattern-based methods}

Methods such as \ac{DNA} amplification through \ac{PCR} or digestion with \ac{REs} can provide accurate and quick results. Moreover, they can be generalized to characterize strains of any bacterial species.

\subsubsection{Pulsed-field gel electrophoresis}

\ac{PFGE} is an electrophoretic technique that applies alternating electric fields at different angles to separate large \ac{DNA} molecules ($10kb-10Mb$) \cite{schwartz_separation_1984, herschleb_pulsed-field_2007}. Prior to electrophoretic separation, \ac{REs} that recognize uncommon motifs are used to cleave the bacterial \ac{DNA}. The distinct banding patterns produced by \ac{PFGE} reflect the \ac{DNA} polymorphisms at the \ac{REs} recognition sites and, ideally, can be uniquely associated to a specific bacterial strain. The resolution of \ac{PFGE} depends on the choice of the \ac{REs} used, with \ac{REs} that recognize long and rare motifs yielding potentially more discriminatory results. The standardization of \ac{PFGE} protocols and creation of pattern databases, such as the one hosted by Pulsenet International\footnote{\url{https://www.pulsenetinternational.org/protocols/pfge}}, were crucial for the wide adoption of \ac{PFGE}. Although widely used, \ac{PFGE} is laborious and the results can be influenced by multiple factors, which hinders reproducibility and interoperability \cite{li_bacterial_2009}. Due to these limitations and to the invention of \ac{NGS}, \ac{PFGE} has been gradually substituted by more accurate and versatile methods based on \ac{WGS}.

\subsubsection{Restriction fragment length polymorphism}

\ac{RFLP} \cite{thibodeau_use_1987, todd_chromosome_2001} allows to differentiate patterns of electrophoresis-separated restriction fragments by Southern Blotting with labeled probes \cite{southern_detection_1975}. The similarity of the patterns of \ac{RFs} can be used to differentiate bacterial strains and infer relatedness. Ribotyping is a variation of \ac{RFLP} that uses probes with conserved domains of \ac{rRNA} genes to differentiate strains based on variable regions flanking the bacterial \ac{rRNA} operons \cite{bingen_use_1994}. The distinct banding patterns used for differentiations are named ribotypes. Since \ac{rRNA} operons are universal, ribotyping is highly applicable. Furthermore, it generates fewer fragments that \ac{RFLP} approaches based on frequently cutting \ac{REs}, enabling easier interpretation of results and establishment of nomenclature and database systems. The potential cost-effectiveness of \ac{RFLP} can be overturned by time and labor-consuming protocols, as well as the requirement for large amounts of high-quality \ac{DNA}, which is not always available.

\subsubsection{Polymerase Chain Reaction}

The \ac{PCR} method, originally developed by Kary Mullis, allows the amplification of any target \ac{DNA} sequence in a sample in a cyclic process to generate a large number of copies of the target sequence \cite{mullis_specific_1986}. \ac{PCR} is performed by temperature cycling, with each cycle having three stages: denaturation, annealing, and elongation. Firstly, high temperature is applied during the denaturation stage to separate the \ac{DNA} strands. Secondly, the temperature is lowered to allow for the annealing of two oligonucleotide primers that flank the target sequence. Lastly, the temperature is raised to the optimum level at which a heat-stable polymerase can extend the primers by incorporating \ac{dNTP}s. Over the years, a plethora of \ac{PCR}-based methods were developed to expand the applicability and overcome limitations of the original \ac{PCR}, firmly cementing \ac{PCR} as the swiss-army knife of molecular biology.
From the vast number of \ac{PCR}-based methods that were invented, some, such as multiplex \ac{PCR} and \ac{qPCR}, are broadly applicable. Multiplex \ac{PCR} \cite{chamberlain_deletion_1988} uses multiple primer pairs to simultaneously amplify multiple target sequences in the same \ac{PCR} reaction.
\ac{qPCR} \cite{higuchi_simultaneous_1992, kubista_real-time_2006} allows to simultaneously amplify target sequences and detect the \ac{PCR} product. This is achieved by incorporating and monitoring the fluorescence of dyes or probes, which increases proportionally to the amount of product formed. \ac{qPCR} overcomes challenges related to product quantification in the original \ac{PCR} and allows for faster confirmation of the presence of a target sequence.
Other \ac{PCR}-based methods were developed for more specific tasks, such as genotyping. Methods such as \ac{AP-PCR or RAPD} \cite{welsh_fingerprinting_1990, williams_dna_1990}, which uses arbitrary primers for random amplification, and \ac{REP-PCR} \cite{versalovic_distribution_1991, de_bruijn_use_1992}, which amplifies regions between interspersed repetitive elements, generate fragment patterns that can function as signatures for specific bacterial strains. The identification of \ac{VNTR} in bacterial genomes through \ac{MLVA} enables to identify polymorphic sites \cite{lindstedt_multiple-locus_2005}. \ac{VNTR} elements evolve rapidly and the number of tandem repeats per locus may vary between strains. \ac{MLVA} uses \ac{PCR} to amplify multiple \ac{VNTR} loci, followed by analysis of the banding pattern to assign a genotype and infer phylogenetic relationships. Although \ac{MLVA} is a simple technique and may offer high resolution, \ac{VNTR} loci in closely related strains may evolve quickly, hindering long-term surveillance. Additionally, \ac{VNTR} may not be common in some species, which limits its applicability, and the accuracy of \ac{MLVA} might be affected by insertions or deletions in the amplified regions.
Some \ac{PCR}-based methods combine \ac{PCR} with other typing methods, such as methods that use \ac{REs}, to overcome limitations and improve accuracy. One example is \ac{PCR}-\ac{RFLP}, which can amplify target regions directly from clinical or environmental samples and uses \ac{REs} digestion of the \ac{PCR} amplicons to generate a limited number of \ac{RFs} that are more easily separated by gel electrophoresis and interpreted \cite{wichelhaus_rapid_2001}. Another example is \ac{AFLP} \cite{vos_aflp_1995}. \ac{AFLP} uses \ac{REs} for digestion of genomic \ac{DNA} followed by ligation of \ac{RFs} with end-specific adapters that allow for selective amplification.
\ac{PCR}-based methods display multiple advantages, such as being relatively inexpensive, fast, and sensitive. Notwithstanding, researchers should be mindful about the inherent limitations of each \ac{PCR}-based method, and of limitations which are common to most \ac{PCR}-based methods, such as the potential for contamination, artifacts, and the need for multiple controls.

\subsection{DNA hybridization-based methods}

\ac{DNA} hybridization-based methods use fluorescently labeled probes, which correspond to known \ac{DNA} fragments, to detect complementary \ac{DNA} sequences extracted from samples \cite{freeman_fundamentals_2000}. The \ac{DNA} arrays used in these methods allow testing for the presence of hundreds to tens of thousands of \ac{DNA} fragments, being useful to study the genetic diversity of bacteria and in transcriptomics. Two types of \ac{DNA} arrays exist: macroarrays \cite{gress_hybridization_1992, lennon_hybridization_1991} and microarrays \cite{derisi_use_1996, schena_quantitative_1995, shalon_dna_1996}. The former can contain up to five thousand spots, providing enough resolution to detect genes involved in \ac{AMR} or for typing methods based on the detection of polymorphisms in a smaller number of loci, such as spoligotyping for MTC bacteria. \ac{DNA} microarrays are more expensive, but provide far greater discriminatory power, including up to tens of thousands of distinct probes, enabling the identification of a greater number of loci compared to macroarrays or to study the variation at the genome or transcriptome level. Since the probes included in microarrays are defined based on reference sequences, microarrays may lack probes complementary to accessory genes, leading to an underestimation of genetic diversity. \ac{DNA} microarrays can use \ac{cDNA} or shorter oligonucleotides as probes. The former are used to determine gene presence, while the latter is capable of detecting smaller patterns of variation, such as deletions or even SNPs. The use of \ac{DNA} arrays has largely been supplanted by the use of \ac{HTS} \cite{bumgarner_dna_2013}.

\subsection{DNA sequencing technologies}

The advent of \ac{DNA} sequencing technologies represented a major milestone in biological research, finally unlocking the genetic information encoded in \ac{DNA}, which had already been established as the source of genetic information in 1944 by Oswald Avery while working with \textit{Streptococcus pneumoniae} \cite{avery_studies_1944} and whose three-dimensional structure was determined in 1953 by Watson and Crick based on the crystallographic data produced by Rosalind Franklin and Maurice Wilkins \cite{watson_molecular_1953, zallen_despite_2003}. The potential and continuous improvement of these technologies contributed to their adoption and led to the development of highly reproducible and accurate methods used to differentiate bacterial strains and identify determinants of phenotypic features of interest. As sequencing throughput and sequence data availability increased, the diverse and highly dynamic nature of bacterial genomes was unveiled, leading to an unprecedented interest in developing sequence-based methods that could probe into the accumulated sequence data to gain new insights. The application of \ac{HTS} revolutionized our understanding of human health and disease by elucidating fundamental biological and ecological processes.

\subsubsection{First-generation DNA sequencing}

Sanger sequencing, also called dideoxy sequencing or chain termination \ac{DNA} sequencing, was the first generation of sequencing technologies \cite{sanger_dna_1977}. This method determines the nucleotide sequence of a single-stranded template \ac{DNA} using a \ac{DNA} polymerase to synthesize nucleotide fragments of different lengths by incorporating radio or fluorescently labeled dideoxynucleotides (ddNTPs) and premature termination of the DNA amplification elongation step \cite{heather_sequence_2016, rodriguez_genesis_2023}. The truncated fragments resulting from the interruption of the elongation step are size-separated by gel electrophoresis to reconstruct the original sequence. As the first successful \ac{DNA} sequencing technology, Sanger sequencing was instrumental in projects such as the sequencing of the first bacterial genome, the genome of \textit{Haemophilus influenzae} \cite{fleischmann_whole-genome_1995}, and the Human Genome Project, which in 2003 achieved the monumental task of determining the first nearly complete sequence of a human genome \cite{international_human_genome_sequencing_consortium_finishing_2004}.
Sanger sequencing was the most widely used sequencing technology until new and cheaper sequencing technologies with greater throughput were developed, such as those based on \ac{SBS}.

\subsubsection{Second-generation DNA sequencing}

Pyrosequencing \cite{nyren_solid_1993, ronaghi_real-time_1996, margulies_genome_2005} was the first second generation \ac{SBS} technology to reach the market. The general principles of second generation \ac{SBS} technologies are the following: i) attachment of the \ac{DNA} to be sequenced to a solid support, usually combined with amplification to enhance signal detection; ii) single-stranded \ac{DNA} synthesis; iii) primer-dependent incorporation of complementary bases; iv) detection of each incorporated nucleotide for sequence determination. Pyrosequencing is based on real-time quantitative detection of pyrophosphate released as nucleotides are incorporated into a growing \ac{DNA} sequence, yielding reads around 400-500 \ac{bp} long. Initially, libraries of \ac{DNA} molecules are attached to paramagnetic beads via adapter sequences and amplified through emulsion \ac{PCR}. Ideally, on average only one \ac{DNA} molecule attaches to each bead so that each bead is coated in a clonal \ac{DNA} population after emulsion \ac{PCR}. The \ac{DNA}-coated beads are distributed into a plate that fits one bead per well where pyrosequencing occurs as bead-linked enzymes and dNTPs are added and the pyrophosphate release is detected by a sensor \cite{heather_sequence_2016, nyren_history_2015, ronaghi_sequencing_1998, nyren_enzymatic_1987}. Compared to conventional Sanger sequencing, pyrosequencing has much higher throughput with a fraction of the cost, making it easier and more viable to scale-up. It does have some limitations over Sanger sequencing, however, as the read lengths are shorter, making downstream analysis, such as assembly, more complex. The most used pyrosequencing technology was 454 sequencing, which had major advantages over traditional Sanger sequencing, as demonstrated by its application to investigate drug resistance in \textit{Mycobacterium tuberculosis} \cite{andries_diarylquinoline_2005} and the whole genome sequencing of Jame Watsonâ€™s genome in record time and within a fraction of the cost of the Human Genome Project \cite{rothberg_development_2008, wheeler_complete_2008}. 454 sequencing was eventually discontinued in favor of more accurate and advanced technologies, such as Illuminaâ€™s \ac{SBS}.
Further improvements to massive parallel \ac{SBS} were introduced with the development of reversible and fluorescently labeled terminators \cite{turcatti_new_2008}. The most widely known sequencing strategy that incorporated these improvements is Illuminaâ€™s \ac{SBS} \cite{uhlen_sequential_2023, bentley_accurate_2008, fedurco_bta_2006}. Illuminaâ€™s \ac{SBS} systems enable \ac{MPS} of small DNA fragments, yielding sequencing reads with up to 250 bp. Illuminaâ€™s \ac{SBS} technology starts by binding adapter sequences to the \ac{DNA} libraries, which contain complementary sequences that bind to the flow cell, unique indexes or barcodes for sample identification, and the sequencing primer binding sites. The \ac{DNA} molecules attached to the flow cell undergo bridge amplification to generate clonal clusters. Sequencing is performed in cycles by using â€˜reversible-terminatorâ€™ dNTPs and detecting the incorporated nucleotides before proceeding to the next cycle. After sequencing the forward strand in this manner, Illuminaâ€™s systems are capable of sequencing the reverse strand, generating \ac{PE} data, which significantly improves the accuracy of downstream analysis. The advantages of Illuminaâ€™s \ac{SBS} systems led to their worldwide adoption and establishment as the dominant sequencing technology for projects of any scale. The higher throughput of Illuminaâ€™s \ac{SBS}, especially as sequencing costs decreased, led to an explosion of sequence data, effectively pushing research into the era of â€˜big dataâ€™ and data-driven research \cite{uhlen_sequential_2023}. The tremendous increase in the number of microbial genomes deposited in public databases in recent years is in great part due to the widespread application of second-generation sequencing technologies, especially of Illuminaâ€™s \ac{SBS}. Other examples of the successful application of this technology are the sequencing of 25,000 cancer genomes by the Cancer Genome Consortium \cite{zhang_international_2019} and the 100,000 Genomes Project \cite{the_100000_genomes_project_pilot_investigators_100000_2021}.

\subsubsection{Third-generation DNA sequencing}

The third-generation of sequencing technologies provide single-molecule sequencing and eliminate the requirement of \ac{DNA} amplification characteristic of second-generation sequencing technologies. Currently, the most successful technologies are HiFi sequencing from \ac{PacBio} \cite{wenger_accurate_2019, eid_real-time_2009} and Nanopore sequencing from \ac{ONT} \cite{mikheyev_first_2014, stoddart_single-nucleotide_2009}. HiFi sequencing works by creating circularized \ac{DNA} libraries that are sequenced in repeated passes to generate several subreads per \ac{DNA} molecule, which can be compared to determine a consensus read minimizing sequencing errors. HiFi sequencing occurs inside small wells on a \ac{SMRT} Cell microchip where \ac{DNA} extension with fluorescent dNTPs is finely monitored. The sequencing technology implemented by \ac{ONT} passes \ac{ssDNA} through a biological nanopore embedded in a synthetic membrane, across which a voltage is applied. The passage of the \ac{ssDNA} through the nanopore limits ionic flow and induces a current change for a period of time that allows to infer the sequence of the \ac{ssDNA} traversing the nanopore. Both technologies generate reads with length that can far exceed the length of the reads generated by second-generation technologies, which is why they are also called long-read technologies. With HiFi sequencing, read lengths can reach 1 to 25 \ac{kb}. Nanopore sequencing is capable of generating even longer reads, from a few to more than a hundred \ac{kb}. Accuracy-wise, HiFi sequencing has the high ground, but the accuracy of nanopore sequencing has been gradually improving because of improvements to the base calling software and the development of superior nanopores.

\subsection{DNA sequencing-based methods}

As high-throughput sequencing technologies became more accurate and cost-effective, the wide adoption of high-throughput sequencing by research and public health institutions became viable. The application of these technologies to help resolve infectious disease events, such as the cholera epidemic in Haiti after the 2010 earthquake \cite{barzilay_cholera_2013} and the international outbreak of Escherichia coli disease linked to contaminated fenugreek sprouts \cite{king_outbreak_2012, mellmann_prospective_2011}, quickly revealed that they were an invaluable tool for the surveillance and outbreak investigation of bacterial pathogens. \ac{WGS} of bacterial isolates yields the complete or nearly complete genome sequence, which in principle encodes all the genetic features necessary for a detailed characterization of an isolate. In principle, this approach, along with powerful bioinformatics tools, allows identifying the relevant features for typing and diagnosis purely based on a sequence approach, effectively replacing many traditional microbiological workflows \cite{besser_next-generation_2018, deurenberg_application_2017}. Moreover, \ac{WGS} may also allow for the identification of emerging genetic features not tested for in routine molecular tests and the detection of bacterial strains that cannot be successfully cultured \cite{deurenberg_application_2017}.
The surveillance of food-borne diseases has greatly benefited from the implementation of standardized \ac{WGS}-based systems. An estimated 600 million people fall ill due to contaminated food annually, resulting in over 400 thousand premature deaths \cite{noauthor_who_nodate}. This puts global public health systems under strain and leads to significant costs related to medical treatment and to productivity and trade losses. Initial reports on the adoption of \ac{WGS} by the PulseNet surveillance network in 2000 demonstrated improved outbreak detection and an increase in the number of solved outbreaks compared to using PFGE data \cite{besser_next-generation_2018, jackson_implementation_2016, ribot_pulsenet_2019}. The gradual adoption of \ac{WGS} by the network participants and the standardization of analytical workflows established \ac{WGS} as the gold standard for typing of foodborne pathogens tracked by the network.
In 2019, the \ac{ECDC} published a strategic framework for the integration of molecular and genomic typing into European surveillance and multi-country outbreak investigation \cite{european_centre_for_disease_prevention_and_control_ecdc_2019}. The document included a progress report on the implementation of \ac{WGS} for surveillance and outbreak investigations by the \ac{EU/EEA} member states and outlined key technological milestones to improve the surveillance and outbreak detection of priority pathogens/diseases.

Through joint work with European the European Food Safety Authority (EFSA)

The widespread implementation of \ac{WGS}-based systems is not without challenges, however. Building \ac{WGS} capacity requires a significant investment in sequencing instruments, reagents, and computational resources to store and analyse \ac{WGS} data. The choice of analytical methods is crucial and may be especially complex, as there are multiple fundamental approaches, which are not necessarily equivalent or comparable.

NGS directly from clinical samples can be challenging due to a disproportionate abundance of host DNA. However, target sequencing of the 16S rRNA gene can provide insight into the microbial composition of clinical samples, aiding in the identification of potential pathogens when linked to infection context.

Sequencing reads are de novo assembled (genome assembly) with genome assemblers such as SPAdes (citation) to determine a set of contiguous sequences, called contigs, resulting from the comparison and combination of overlapping reads. Overall, the greater the length of the reads and the accuracy of the base calling, the smaller the number of contigs and the larger they are.
Strain similarity can be investigated through gene-by-gene methods, such as by expanding the classical multilocus sequence typing (MLST) approach to deal with bigger sets of genes, such as focusing on the conserved core genes (cgMLST), or on the entirety of the gene content (wgMLST), which includes the accessory genes. This can be achieved with software such as chewBBACA and SeqSphere, and with online platforms, such as Enterobase and BIGSdb (citations). The adoption of MLST schemas also allows establishing allelic nomenclatures to compare results at the inter-laboratory level.
The data throughput and resolution provided by WGS has promoted its adoption for public health. It has been successfully applied for surveillance and outbreak detection by monitoring chains of transmission and the dissemination of resistance and virulence factors. WGS-based phylogenetic analysis provides richer evolutionary context to identify emerging lineages and associated genetic markers. By providing access to the full or majority of the gene content, WGS also allows for a retrospective analysis to find closely-related strains and relationate with epidemiological data for a more effective identification of chains of transmission.

Long-read sequencing can be used to generate scaffolds to map short-read data against, combining read length of long-read sequencing and the read depth and accuracy of short-read technologies [34].

\subsubsection{Multilocus sequence typing}

\ac{MLST} is a sequence-based approach that uses allele fragments, typically seven, from housekeeping genes to characterize microorganisms, with more expressive application for bacterial species of pathogenic potential. \ac{MLST} is based on the principles of \ac{MLEE}, but uses nucleotide sequences at each locus, taking advantage of developments in sequencing technologies and bioinformatics \cite{urwin_multi-locus_2003}. Moreover, \ac{MLST} allows to identify a greater number of alleles per locus, offering higher discrimination than \ac{MLEE} while using a smaller number of loci. \ac{MLST} was initially developed to better accommodate signals of vertical and horizontal genetic transfer and overcome challenges of traditional and molecular typing methods, such as the inability to infer strain relatedness and poor reproducibility within and between laboratories \cite{maiden_multilocus_1998}.

The distinct fragments identified at each locus are assigned unique integer identifiers in order of discovery and the combination of identifiers for the allele fragments identified in all loci constitute an allelic profile, which can be compared against a database of known allelic profiles. Each distinct allelic profile unambiguously defines a \ac{ST}, assigned to isolates for direct comparisons. \ac{ST}s are grouped into \ac{CC}, a concept first introduced to describe \textit{N. meningitidis} isolates analysed by \ac{MLEE}, based on their similarity to a central \ac{ST} (allelic profile or genotype). The definition of central \ac{ST}s is achieved through a combination of computational and experimental data obtained from public health authorities. Newly identified \ac{ST}s are assigned to the most similar \ac{CC} based on a minimum number of shared alleles with the central \ac{ST}. \ac{ST} organization into \ac{CC}s facilitates epidemiological analysis, often grouping most \ac{ST}s into a much smaller number of \ac{CC}s and allowing to identify \ac{CC}s of greater clinical relevance. One disadvantage of \ac{MLST} is that it may not offer the same degree of discrimination within lineages or species with highly uniform housekeeping genes. Additionally, due to the diversity of bacterial species, \ac{MLST} schemes must be developed to distinguish closely related bacteria, usually at the genus and species levels, or they may not provide sufficient resolution. Consequently, \ac{MLST} cannot be applied as a combined taxonomic and typing approach at all levels of bacterial diversity \cite{jolley_ribosomal_2012}.

By relying on the sequencing of allele fragments from multiple chromosomal locations, \ac{MLST} provides unambiguous results and is more robust to recombination events, constituting a faster and more sensitive technique than most laborious lab protocols, which also tend to be more unpredictable as variation accumulates. Since allele fragments are used as a unit of comparison, single allele differences constitute a single event, regardless of the number of nucleotide polymorphisms involved. While this model may not provide resolution for every single point change, it is resistant to horizontal genetic transfer events, which introduce a lot of variation in a single event, leading to an inaccurate estimate of similarity if counted as single differences.

\ac{MLST} aims to provide good discrimination for short and long term epidemiology, with the original study showing that it was congruent with and more discriminatory than \ac{MLEE} in distinguishing hyper-virulent strains of \textit{Neisseria meningitidis} while also offering a clear distinction between lineages at the species-level. A subsequent study presented a \ac{MLST} database for \textit{Streptococcus pneumoniae}, obtaining consistent results with \ac{MLEE} and \ac{PFGE} for the analysis of predominantly invasive and antibiotic-resistant isolates. Moreover, \ac{MLST} was also congruent with serotyping, with isolates sharing the same or similar \ac{ST}s also expressing the same serotype, except for cases where recombination at the capsular locus was suspected to have led to capsular switching \cite{enright_multilocus_1999}.

Numerous \ac{MLST} databases have been developed since \ac{MLST} was proposed. Currently, a collection of public, curated, and frequently updated \ac{MLST} databases is available for a great number of microbial species on the PubMLST website\footnote{\url{https://pubmlst.org/}}. PubMLST integrates sequence data with sample metadata to promote the exchange of molecular typing data for epidemiological studies. As of 27 March 2025, PubMLST manages more than 130 species and genera-specific \ac{MLST} databases, which contain tens of millions of alleles identified and submitted by researchers. The sheer volume of data and the range of databases in PubMLST highlight how the advantages of \ac{MLST} contributed to its rapid adoption worldwide, with the technique widely used for epidemiological studies, to identify localized disease outbreaks and monitor local and global trends, and for population studies, to examine the structure of bacterial populations and perform evolutionary analyzes.

\ac{MLST} improved resolution, reproducibility, and portability compared to other typing methods used at the time.

(Add image with location of MLST genes for N. meningitidis and S. pneumoniae?)

\subsubsection{16S rRNA gene typing}

The integration of the 16S \ac{rRNA} gene into bacterial systematics allowed the establishment of a classification system based on evolutionary relationships \cite{jolley_ribosomal_2012, woese_towards_1990}. The ubiquity and conserved nature of the 16S \ac{rRNA} gene makes it an effective target for bacterial systematics, as evidenced by its seminal role in establishing the place of bacteria in the domains of life \cite{woese_bacterial_1987}, and for the compositional study of bacterial communities, such as the human microbiome and environmental samples. While the 16S \ac{rRNA} gene serves as the basis for a general framework for phylogeny, it falls short when strains with distinct features as determined by other typing methods share identical or very similar 16S \ac{rRNA} gene sequences. This limits its role as a proxy to reliably identify groups of pathogenic bacteria with relevant properties for diagnosis, treatment and epidemiology. Currently, it is mostly used for bacterial species identification and for compositional and abundance estimation of complex samples.

(talk about Greegenes, Silva databases and software such as QIIME and Kraken 2)

\subsubsection{rMLST}

\ac{rMLST} typing indexes the variation of the genes encoding the bacterial \ac{rps}. The \ac{rps} genes are ideal targets for universal bacterial characterization because they are: i) universally present; ii) distributed across the genome, which makes \ac{rMLST} more robust against horizontal gene transfer events that reassort loci and break phylogenetic congruence; and iii) encode proteins which are functionally conserved across the Bacteria domain. \ac{rMLST} constitutes a combined taxonomic and typing approach for the whole domain of Bacteria at all taxonomic levels. \ac{rMLST} allelic profiles or \ac{rSTs} determined through \ac{rMLST} provide a basis for universal bacterial systematics, allowing for a precise identification of the phylogenetic position at any taxonomic rank, while also distinguishing closely-related strains for typing purposes. A database for the 53 \ac{rps} genes identified in bacteria is managed by the \ac{BIGSdb} platform \cite{maiden_mlst_2013}.

\subsubsection{wg/cgMLST}

The level of resolution for typing depends on the desired application. Higher resolution is necessary for the detection of outbreaks and within-patient variation. On the other hand, lower resolution is required to group strains into \ac{CC}s or lineages. The \ac{GbG} approach is inherently hierarchical and scalable, meaning that the number of genes used in the analyzes can be adjusted based on the desired resolution \cite{maiden_mlst_2013}. Thus, the concept and analysis methods of the highly successful seven-gene \ac{MLST} can be intuitively scaled to hundreds or thousands of genes to encompass diversity at the core- or whole-genome level, giving rise to \ac{wg/cgMLST}. \ac{wg/cgMLST} provides higher resolution for surveillance and outbreak investigation. Furthermore, the additive nature of \ac{MLST}, through the continuous update of schemas with novel alleles, ensures that \ac{wg/cgMLST} can provide accurate results in the long-term while also promoting interoperability.

\ac{cgMLST} characterizes bacterial strains based on the identification and comparison of the genes that constitute the core genome. Although the core genome is often defined as the set of genes present in all strains of a given dataset, a more relaxed definition is needed to account for technical and biological variation. A loci presence threshold of 95\% is commonly used to accommodate biases and errors introduced by, for example, the sequencing and genome assembly processes. This allows to retain genes that are present in almost all strains of a species or that are reported as absent due to misassembly. Furthermore, the set of core loci is usually determined based on the allele calling results for a specific dataset ideally representative of the diversity of the species. This means that the definition of core genome is highly dependent on the dataset used and the number of core loci that are detected varies according to the dataset.

\ac{wgMLST} further expands the set of loci used for strain typing by including loci from the accessory genome. The frequency of accessory loci is highly variable, with some accessory loci being nearly as frequent as core loci, and others being found only in a small subset of strains or even being strain-specific. In theory, a \ac{wgMLST} schema should contain more loci than a \ac{cgMLST} schema, but there are no hard requirements regarding the fraction of core and accessory loci of a species that should be included, allowing for \ac{wgMLST} schemas with a number of loci close to that of a \ac{cgMLST} schema or considerably larger \ac{wgMLST} schemas encompassing all known core and accessory loci for a species. Since \ac{wgMLST} identifies more loci than \ac{cgMLST}, it provides greater resolution and is potentially more discriminatory when there is variation in the accessory genome. However, creating \ac{wgMLST} schemas requires more careful selection of target loci compared to \ac{cgMLST} schemas. While \ac{cgMLST} schemas include only core loci, which usually display lower allele diversity, the inclusion of accessory loci in \ac{wgMLST} schemas increases the frequency of spurious loci due to sequencing and assembly errors or to real sequence variability, affecting the accuracy of the results. Since \ac{cgMLST} provides robust results and the majority of the schemas that are available are \ac{cgMLST} schemas, most analyses are performed at that level, with \ac{wgMLST} being recommended when higher resolution is necessary, such as when comparing closely related strains for surveillance and outbreak detection (cite mixao and heather articles).

\ac{wg/cgMLST} characterizes strains by determining their allelic profiles (i.e., the set of loci and alleles identified in each strain). The comparison of the allelic profiles to determine the number of shared loci and alleles provides an estimate of strain similarity. This can be done by discarding missing loci from the analysis or by computing the absolute difference. The former is preferred, as genome fragmentation and potential sequencing and assembly errors make it impossible to be certain if a locus that was not identified is in fact absent from the genome. The cross-comparison of a set of samples allows to compute a distance matrix including the number of allelic differences between each pair of strains. The distance matrix enables phylogenetic analysis through methods such as single-linkage clustering, \ac{NJ} or by computing a \ac{MST}. Computing a \ac{MST} is frequently used in surveillance and outbreak-investigation scenarios as it provides accurate results when comparing closely related strains. For the study of more diverse populations, more robust results can be obtained through methods such as maximum likelihood after aligning the alleles identified in all strains for each locus and concatenating the alignments to get a core genome \ac{MSA}. There are multiple software, often available through web platforms, to generate and visualize trees from \ac{wg/cgMLST} results and overlay the tree with metadata to more easily identify relevant strains and associations.

A distance threshold can be defined to identify groups of highly similar strains corresponding to lineages or potential outbreaks. Threshold definition is often empirical, depending on the diversity of the species, dataset, context, and methods used for the analysis. Consequently, threshold values are not universally applicable, which hinders comparability of the results obtained in different settings (e.g. outbreak detection in different countries.

\paragraph{wg/cgMLST platforms} \mbox{}\\

There are multiple web platforms that store and manage \ac{wg/cgMLST} schemas. These platforms provide access to \ac{wg/cgMLST} schemas for a wide range of species and offer different functionalities for data analysis. All well-established \ac{wg/cgMLST} platforms centralize data analysis by requiring users to upload their data. Platforms such as \ac{BIGSdb} and Enterobase operate under more permissive licenses, providing unrestricted access to schemas and functionalities upon registration and allowing other users or institutions to set up their own instances of the platform. Other platforms, such as Ridom SeqShere+, are proprietary software that requires users to pay for access to the platform's functionalities, although the schemas are usually publicly available. The results generated within different platforms are not directly comparable as schemas for the same species stored by different platform may target different sets of loci, and use distinct loci and allele nomenclatures, which hinders interoperability. The results are not easily comparable even when the schemas have the same origin and use the same nomenclature, as each platform applies different methods for allele identification and the nomenclatures are not synchronized, meaning that the same allele can have a different identifier in each platform (cite paper about differences in typing results due to methods). \ac{BIGSdb} was the first platform to enable \ac{wg/cgMLST} and is a prime example of a solution for \ac{wg/cgMLST} that has been widely adopted and offers extensive analytic capabilities \ac{BIGSdb} extended the functionalities for \ac{MLST} of the PubMLST platform to \ac{WGS} data \cite{jolley_bigsdb_2010, jolley_open-access_2018}. \ac{BIGSdb} pioneered the application of \ac{GbG} methods to genome analysis, storing genomic and gene sequences, as well as associated metadata, such as provenance and phenotypic data for the isolates the sequence data originated from. Additionally, it stores allele and locus definitions, without an inherent limit to the number of records or the number of schemas the loci can be grouped into. The loci included in a schema do not need to be associated to a single organism, enabling the creation of schemas that encompass the diversity of genes, such as accessory genes, that are distributed in diverse organisms. Known and novel alleles are identified from sequence data uploaded to \ac{BIGSdb} to maintain a record of the known diversity of genes identified in the samples stored in the database. Furthermore, genomic data is rescanned periodically as the database expands to identify variants in the stored isolates that could not be detected previously based on the represented allele diversity in the database. The functionalities included in \ac{BIGSdb} allow users to link isolate and sequence data with great flexibility, enabling the definition of schemas encompassing the diversity of the species with utility for epidemiological investigations and population analysis or smaller schemas to study particular aspects of the biology of an organism. The genetic nomenclatures established and maintained by \ac{BIGSdb} enable the definition of classification hierarchies for an effective comparison of bacterial isolates globally \cite{jolley_open-access_2018}.

Hierarchical nomenclature definitions are artificial constructs that are useful to systematize and communicate results. The definitions should be stable, but flexible at long-term and capable of accommodating results data matching different levels of resolution (e.g. seven-gene MLST and wg/cgMLST).

\subsubsection{SNP-based methods}

An accurate estimation of strain similarity for phylogenetic analyzes can be achieved by comparing the genomes of strains of interest against the genome of a reference strain to identify \ac{SNPs}. This is performed by mapping sequencing reads against a reference genome to identify all variable positions in regions shared with the reference, identifying the set of core \ac{SNP}s. Similarly to \ac{wg/cgMLST}, pairwise \ac{SNP} distances or the alignment of the core \ac{SNP}s can be determined to perform phylogenetic analysis with methods such \ac{NJ} or maximum likelihood, respectively. Since SNP-based approaches identify variation at the single nucleotide level, they can potentially provide greater resolution than \ac{wg/cgMLST} for the regions being considered. However, it is necessary to meet two requirements for high-resolution typing with SNP-based methods. First, choosing a reference genome closely related to the strains under investigation is of the utmost importance, as \ac{SNP}s are only identified in the regions shared with the reference. A more divergent strain may considerably reduce the number of shared regions, and consequently the number of detected \ac{SNP}s. Strategies for choosing a reference genome include selecting a high quality genome from the same \ac{ST}, \ac{CC}, serogroup or determining the genome distance to a group of potential reference genomes, such as the \ac{ANI}. Second, the group of strains under investigation cannot be very diverse, as that will also reduce the number of regions considered for \ac{SNP} determination \cite{jolley_bigsdb_2010}. These requirements can be met for outbreak detection and investigation scenarios in which SNP-based methods have been successfully applied to resolve national and international outbreaks. However, these requirements limit the applicability of SNP-based methods compared to \ac{wg/cgMLST} approaches, which can be used to characterize from very diverse datasets to closely related strains. In addition, SNP-based methods are less robust to recombination and \ac{HGT} than \ac{wg/cgMLST}, as a single event will lead to the identification of multiple \ac{SNP}s, potentially overvaluing the distance between strains. The congruence between SNP-based and \ac{wg/cgMLST} approaches is highly dependent on the reference genome and schema used, respectively, as well as the parameters used and the dataset under analysis. While some studies have reported good congruence between both approaches for outbreak analyses, others have highlighted that the results generated with systems are not directly comparable and a congruence analysis is necessary to assess method equivalence \cite{mixao}. Furthermore, SNP-based approaches do not scale as well as \ac{wg/cgMLST}, in part because they are more computationally demanding as the dataset increases, but also because it may be necessary to use multiple references and fine-tune the parameters for accurate SNP detection, making it harder to establish a reference database for consistent results when compared to \ac{wg/cgMLST} schemas. However, there is software that stores variant calling data, such as SnapperDB \cite{} in a database for future comparisons.

\subsubsection{\textit{k}-mer-based methods}

Although the concept of \textit{k}-mer has existed for several decades, even if under different designations (e.g. N-gram, k-tuple, w-mers, ), its wide application to increase the efficiency and accuracy of bioinformatics methods is relatively recent. \textit{k}-mer-based approaches break sequences into smaller subsequences of length \textit{k}. This seemingly simple approach of \textit{break it to understand it} has enormous potential, allowing for much more time-efficient sequence comparisons than alignment- or mapping-based approaches. However, storing large sets of \textit{k}-mers in memory can lead to high memory usage. Thus, it is important to optimize parameters such as the \textit{k}-mer size and sampling method, also termed sketching (i.e., which \textit{k}-mers to select from all possible \textit{k}-mers generated from a sequence). With respect to \textit{k}-mer size, it is important to optimize it for the desired application by balancing the trade-off between greater specificity achieved with longer \textit{k}-mers and greater sensitivity with shorter \textit{k}-mers. Optimizing k-mer selection is more complex and has been the subject of extensive research \cite{}. Ideally, it is desired to select the smallest set of \textit{k}-mers that minimizes memory usage without sacrificing accuracy. However, depending on the application, it may be necessary to satisfy other requirements that complicate the determination of the optimal sampling method. For example, a sampling method that selects \textit{k}-mers randomly may be preferred for unbiased sequence comparisons, while optimizing the interval between consecutively selected \textit{k}-mers may offer better performance when sequence variability is higher. Theoretical and empirical evaluations of the performance of each sampling method are important to determine optimal parameters and limitations. Minimizers and spaced seeds are two sampling methods that have been widely applied due to their simplicity and effectiveness in a wide range of applications, such as genome assembly and taxonomic classification. \textit{k}-mers are a centerpiece of most genome assemblers, with the construction of \textit{de Brujin} graphs, representing the overlap between \textit{k}-mers determined from sequencing reads, being the fundamental strategy used by many genome assemblers to solve the problem of genome assembly for both short- and long-read data \cite{medvedev_what_2021}. For taxonomic classification, sequence data are often decomposed into \textit{k}-mers and compared against \textit{k}-mer indexes constructed from taxonomically annotated reference sequences. This strategy allows for ultra-fast taxonomic classification. Kraken was one of the first highly successful \textit{k}-mer based tools for taxonomic classification \cite{wood_kraken_2014}. Its first version enabled fast taxonomic classification based on a minimizer index, and the second version optimized the database structure to reduce memory requirements and added spaced seeds for greater accuracy \cite{wood_improved_2019}.

In theory, \textit{k}-mer-based methods offer several advantages over \ac{wg/cgMLST} and \ac{SNP}-based methods. Firstly, there is no need to define a reference database, such as a schema, or select a reference genome to compare strains against. This overcomes the reference bias limitation of \ac{SNP}-based approaches and eliminates the need to store and manage a schema with a specific allelic nomenclature. However, to achieve consistent results, especially when trying to establish a cluster nomenclature, it is still recommended to create a database structure to compare and classify strains. Otherwise, the results obtained for different datasets may not be comparable. Secondly, and contrary to \ac{wg/cgMLST}, \textit{k}-mer-based methods can include non-coding regions to estimate strain similarity. Lastly, if adequately parameterized, \textit{k}-mer-based methods can be faster and more efficient than \ac{wg/cgMLST} or \ac{SNP}-based methods. A good example of an efficient and versatile \textit{k}-mer-based method applicable to closely related samples and outbreak investigation is SKA2 \cite{derelle_seamless_2024}. SKA2 uses split \textit{k}-mer analysis for reference-free and reference-based mapping to genotype bacterial strains using sequencing reads or genome assemblies. Another \textit{k}-mer based tool, PopPUNK \cite{lees_fast_2019}, uses \textit{k}-mers to calculate core and accessory distances, which in turn are used by machine learning algorithms to cluster bacterial strains. This approach has proved useful for population analysis, and the ability to update existing clusters with new strains without having to recalculate all pairwise distances is especially useful for surveillance.


\section{Aims of the Thesis}

The advances in \ac{DNA} sequencing technologies have allowed research and public health institutions to gradually switch from classical and more laborious microbiological workflows to \ac{WGS}-based methods for the characterization of bacterial pathogens. With modern sequencing technologies, it is possible to streamline the genome sequencing of dozens of bacterial strains. The widespread adoption of \ac{WGS} and the increased availability of bacterial genomes in public databases enabled researchers to develop and apply more advanced bioinformatics methods to gain greater insight into the structure, diversity and dynamics of bacterial genomes. \ac{GbG}, \ac{SNP}-based and \textit{k}-mer-based methods allow for a more detailed characterization of bacterial pathogens and are currently widely applied for surveillance, outbreak investigation, and to study the diversity of bacterial species. \ac{GbG} methods in particular, such as \ac{wg/cgMLST}, have been adopted more frequently for surveillance and outbreak investigation, perhaps in part because they constitute an expansion of the classical seven-gene \ac{MLST}, making it conceptually and technically easier to implement and transition to. The wide adoption of \ac{wg/cgMLST} makes it relevant to explore ways in which it can be further improved. The main objective of this work is to explore concepts and implement solutions that improve the efficiency, accuracy, and interoperability of \ac{wg/cgMLST}. The main goals are the following.

- Optimize the processes of schema creation and allele calling in \ac{wg/cgMLST}, as well as implement solutions for comprehensive analysis of the schema and results data;

- Implement a Nomenclature Server to store and manage \ac{wg/cgMLST} schemas that enables local and private analyzes based on a common allelic nomenclature;

- Identify and propose solutions for common problems found in \ac{wg/cgMLST} schemas, either derived from low-quality data or from limitations of current methodologies used in \ac{wg/cgMLST}.

The first goal will be achieved mainly through the optimization of the chewBBACA software for \ac{wg/cgMLST}. The second goal will focus on creating a Web service that provides easy access to \ac{wg/cgMLST} schemas and minimizes scalability and data privacy concerns compared to other well-known \ac{wg/cgMLST} platforms that centralize data analysis and require users to share their data. The last goal will help create strategies to identify and correct spurious loci added to \ac{wg/cgMLST} schemas, improve the integration of accessory loci into \ac{wgMLST} schemas, and provide valuable information to guide the development of chewBBACA.
