\renewcommand*{\thefootnote}{\arabic{footnote}}

\mbox{}\\
\vspace{8cm}

\section{A brief note on software development practices}

I start this discussion by mentioning a subject that tends to be overlooked in bioinformatics: good practices in software development. After the initial thrill of coming up with a concept for a tool and implementing a rudimentary proof-of-concept, it is time to tidy up. Developing bioinformatics software that is efficient, maintainable, and reliable is the shortest route to contribute to its wide adoption and longevity. The process and effort of writing well-structured and documented code can take up as much time as the algorithmic intricacies, but it is essential for long-term support. While I think it is not necessary to follow each piece of advice given by Uncle Bob\footnote{\url{https://en.wikipedia.org/wiki/Robert_C._Martin}}, defining a handful of simple requirements for good code quality can help avoid turning something simple into a convoluted mess. While working on chewBBACA 3 and Chewie-NS, presented in Chapters \ref{ch:paper1} and \ref{ch:paper2}, I strove to maintain good code quality standards, as I knew that that could greatly influence long-term support. In chewBBACA, the compartmentalization of the code into multiple modules containing smaller functions played an important role in organizing the code to maximize reusability and maintainability. When evident, functions were added to modules based on more general categories, such as functions for file operations (reading and writing files), sequence manipulation and clustering (processing and clustering of DNA and protein sequences), and multiprocessing operations (functions to divide and process inputs in parallel to improve efficiency). Compartmentalization helped to keep track of the set of functions that I had already implemented, reducing the tendency to reimplement functions in distinct parts of the code, and providing a collection of reusable functions interconnecting the code and making the codebase more coherent. When development stretches over long periods, as was the case for chewBBACA 3, often forgetful developers (!) or new contributors will have difficulty understanding the code if it is not well documented. That is why I included \textit{docstrings} in almost every function in chewBBACA 3 and added comments to code parts whose function may seem cryptic or when it is important to explain design choices. The code documentation made it easier to jump right into development and avoid adding redundant or code-breaking changes by better understanding what the code was doing and the parts that needed to be improved or expanded. From an end-user perspective, especially for non-technical users, usage documentation is more important than code documentation. A well-structured and well-written usage documentation helps users quickly understand how to start using software and explore its functionalities. In that regard, I created usage documentation for chewBBACA 3 and Chewie-NS, with quick start guides, step-by-step tutorials, and detailed instructions about each module's functionalities or for the API endpoints and UI in the case of Chewie-NS. Another practice that promotes reproducibility and sustainability is software testing, as regularly testing the functionality of software, especially if automated, can help assess software performance and detect unexpected issues. Testing is a critical, but underused, aspect of scientific software development that I and others have advocated in a publication \cite{van_der_putten_software_2022}. I implemented automated standard functionality testing for chewBBACA 3 to ensure that it produces the expected results and detect issues arising due to minor changes or the introduction of new functionalities. Ultimately, I think that learning and upholding good practices for software development is crucial to develop quality software and maximize its applicability in the long-term, as well as being a valuable skill for any developer. For Pythonistas like myself, it is worth it to once in a while open a Python console and type \textit{import this}\footnote{\url{https://peps.python.org/pep-0020/}}.

\section{Gene prediction and allele identification in wg/cgMLST}

Accurate identification of alleles is of the utmost importance in wg/cgMLST. This is usually done by aligning reference alleles to genome assemblies to identify similar alleles or through gene prediction tools that scan genome assemblies to predict genes based on a set of parameters. Prodigal is a well-known \textit{ab initio} gene predictor for bacterial genomes that was used by older versions of chewBBACA. However, Prodigal has not been actively maintained, raising concerns about its long-term support. As part of the work developed in Chapter \ref{ch:paper1} to improve chewBBACA, I switched Prodigal by Pyrodigal. Pyrodigal provides Python bindings to Prodigal, improving the performance and offering greater control over the gene prediction process, while also ensuring results consistent with Prodigal's last stable version. Although the speed and efficiency of the gene prediction process in chewBBACA 3 has not been directly measured, it is safe to assume that Pyrodigal, whose authors claim that it speedup gene prediction by up to 50\%, contributed to chewBBACA 3's efficiency compared to its previous versions. However, the true value of switching to Pyrodigal lies in the control over the gene prediction process. With Prodigal, gene prediction was only possible by calling Prodigal directly. In contrast, Pyrodigal provides Python bindings that enable fine control of the gene prediction parameters. This allowed not only to seamlessly integrate gene prediction to obtain results consistent with previous chewBBACA versions, but also to plan future changes that may significantly improve the gene prediction process compared to the chewBBACA version presented in Chapter \ref{ch:paper1}. Possible improvements include selecting from five possible file formats for the output file with gene prediction results, which currently can only be written in FASTA format. In addition, the scores attributed to each CDS can be accessed before writing the results, which, through a thorough validation process, can enable the definition of a score threshold used to exclude spurious \ac{CDSs} predicted by Pyrodigal, leading to improvements in allele calling and a reduction in the number of spurious loci included in the schema seeds created by the \textit{CreateSchema} module. The prediction of spurious CDSs is one of the disadvantages of using a \textit{ab initio} gene predictor, especially when the predicted CDSs do not match any known gene, making validation difficult, and when there are systematic errors, such as the prediction of CDSs with an offset in \textit{M. tuberculosis}. Another disadvantage is that it may predict different CDSs from alignment-based gene prediction, the strategy used by widely used platforms such as BIGSdb, which hinders interoperability when users want to use schemas from those platforms with chewBBACA or compare allele calling results. Adding alignment-based gene prediction to chewBBACA may help refine the results provided by Pyrodigal, while also allowing users to define parameters that approximate those used by other wg/cgMLST platforms, promoting interoperability and the further adoption of chewBBACA.

\section{Classification of known alleles}

The high \ac{ANI} values between the genomes of the species datasets used to evaluate chewBBACA 3 in \textbf{Chapter \ref{ch:paper1}} reflect the composition of the public databases from where the genomes were sourced \cite{oleary_reference_2016, blackwell_exploring_2021}. Through the analysis of datasets used in other studies and testing datasets created to evaluate chewBBACA's performance, I observed that, for most species, the degree of inter-species similarity in available genome collections was also high, on average. This observation is supported by multiple studies that indicate \% ANI as the average for species-level...(CITE SOME ANI PAPER!). Genomes with highly similar sequences may share a significant fraction of identical \ac{CDSs}. Thus, using alignment to compare loci in a wg/cgMLST schema with genomes in a pairwise fashion can be inefficient and redundant if the process keeps identifying the same \ac{CDSs}.

The first published version of chewBBACA already determined if the \ac{CDSs} predicted from the input genomes were in the schema without using alignment, but would do this for every single genome, even if the same \ac{CDS} was found in all genomes. To further optimize the exact matching process, I decided to implement a deduplication step after the gene prediction step. This deduplication step identifies the set of distinct \ac{CDSs} identified in all input genomes, returning a hash table with \ac{CDS} SHA-256 hashes as keys and the list of genomes containing each \ac{CDS} as values. The SHA-256 hashes are used as unique identifiers for the exact matching between the \ac{CDSs} and the schema allele hashes, and are shorter than the actual \ac{CDS} sequences, which reduces memory usage compared to using the sequences as keys. Associating the list of genomes with a \ac{CDS} to the \ac{CDS} SHA-256 hash allows identifying and classifying all genomes with a particular \ac{CDS}, avoiding redundant comparisons. An issue that I identified while testing \ac{CDS} deduplication with large datasets was that the lists of genome string identifiers associated to each key could contain many values and occupy a lot of memory, which meant that storing the hash table in memory could become impractical for systems without a lot of \ac{RAM}. Mapping the genome string identifiers to unique integer identifiers and using lists of integer identifiers as values reduced memory usage but was not enough to drop memory usage to the range of values adequate for most users to be able to perform large-scale analysis. To further reduce memory usage, I used modified polyline encoding (CITE NUMCOMPRESS) to compress the lists of genome integer identifiers, yielding strings that occupy less memory than the original lists, and allowing large-scale analysis with the memory available in most modern laptops, as reported in Chapter \ref{ch:paper1} and testing during development.

Although implementing a solution for exact matching that integrates sequence deduplication, sequence hashing, and modified polyline encoding is more complex than simply comparing the CDSs identified in each genome against a \ac{wg/cgMLST} schema, the gains in efficiency far outweigh the increase in complexity since it allows chewBBACA to be used at a scale that was not previously possible.

One possible limitation of this approach is that it assumes that the CDSs identified in larger genome collections are highly repeated. This is in part true for single-species datasets. If it is applied at higher taxonomic ranks, such as at the genus level, sequence diversity may be considerably higher, increasing the size of the hash table and, consequently, memory usage. Performance at the genus level has not been tested in the results presented in this thesis, but there have been applications of wg/cgMLST at that level, such as evidenced by the \textit{Brucella} and \textit{Shewanella} schemas available on Chewie-NS, suggesting that there have been successful applications at the genus level. Applications above the genus level are not envisioned, mainly because the degree of sequence divergence between taxa at higher taxonomic levels may compromise the operational definition of loci used in wg/cgMLST. Another possible limitation is that sequencing tends to be used in studies or interventions deemed more relevant, such as for the sequencing of bacterial species with a significant clinical or economical impact, leading to an incomplete representation of the diversity of bacterial species. In the event that future sequencing experiments begin to reveal more of the diversity of bacterial species, an increase in sequence diversity would also result in an increase in memory usage.

It is important to note that any concerns about increased memory usage may be minimized in the near future as the computational resources available to the general public become more and more powerful.

\section{Classification of novel alleles}

The most common way of identifying loci alleles in bacterial strains is perhaps by aligning a representative allele against assembled genomes. This is usually performed with a sequence aligner, such as \ac{BLAST} \cite{camacho_blast_2009}, and by defining identity and coverage thresholds for the identification of \textit{bona fide} alleles. Although alignment-based approaches work for most cases, they may have some limitations, especially when comparing alleles with significant sequence divergence, either due to accumulation of point mutations and small indels or caused by recombination or \ac{MGEs}. For instance, the extension of local alignments may halt due to significant sequence divergence, yielding local alignments that, while having a high score, do not meet the identity and coverage criteria enforced by the allele callers. This may result in the inability to identify loci that are, in fact, present in genomes. In the specific case of \ac{BLASTp}, divergence in a single sequence region can halt alignment extension between alleles that are otherwise highly similar. In these cases, \ac{BLASTp} can return single or multiple local alignments between alleles that separately, and in the case of chewBBACA, do not reach the \ac{BSR} threshold or would have to be combined to determine if the \ac{BSR} computed from the combined raw alignment scores is sufficient. BLAST is used to determine local alignments, so it seems reasonable that it does not provide any option to combine multiple local alignments to get a combined score. Initially, and perhaps naively, I thought it may be possible to implement a simple script to compute the combined score. But the lack of knowledge about how the alignment raw score is computed, especially with compositional adjustment, made it seem like I was trying to probe a black box. Thus, it appeared reasonable to consider other options. At first glance, one of the options could be to compute global alignments. Although global alignments would cover the full length of the sequences being compared, two factors contributed to ruling it out nearly immediately.

Firstly, global alignment yields different results than local alignment and I would have to use a different aligner than BLAST. This would lead to completely different results, invalidating the BSR-based approach used in chewBBACA, and potentially diverge greatly from the results determined with BLAST, which would warrant a complete overhaul of chewBBACA's algorithm, both conceptually and algorithmically. Secondly, global alignment is computationally expensive and is not adequate to align large collections of potentially divergent sequences. Since I intended to implement a new version of chewBBACA tailored for scalable and efficient analysis of large genome collections, I decided to complement the alignment-based strategy used by chewBBACA with alignment-free methods, more specifically, k-mer-based methods.

The concept of k-mers, although seemingly simple, offers extreme versatility by simply varying the value of \textit{k}, which defines the size of the k-mers, and choosing a sampling method to select subsets of k-mers from a sequence. Research into the optimization of k-mer-based methods has produced concepts such as spaced seeds, minimizers, syncmers, and, more recently, strobemers. These concepts are applicable to virtually any step of the bioinformatics workflow, contributing to the efficiency and accuracy of methods for read mapping, sequence alignment, taxonomic classification, and metagenomic binning, among others.

I chose minimizer-based clustering to complement the alignment-based strategy used by chewBBACA because minimizers are simple to implement, fast to determine, and enabled me to implement a clustering method that was accurate enough for the application I had in mind. Performing intra-cluster alignment to classify the CDSs is much faster than all-vs-all or pairwise alignment between the CDSs identified in all inputs. Minimizer-based clustering also enables the identification of highly similar sequences with differences concentrated in a single or a few sequence regions because it counts the number of shared minimizers along the complete sequences, whereas BLAST may only report smaller local alignments that make it more difficult to determine global sequence identity. The strength of the strategy lies then in combining alignment-based and alignment-free methods to improve speed and accuracy, an approach that has been increasingly used in bioinformatics. Although minimizer-based clustering enabled considerably faster and slightly more accurate wg/cgMLST with chewBBACA, it is not sufficiently sensitive to identify some of the most divergent alleles within the $0.6\geq BSR > 0.7$ interval. Due to this limitation, the classification of the most divergent alleles in chewBBACA 3 uses only BLASTp, which can increase the runtime with larger and more diverse datasets. An alternative approach would be to accept a slight reduction in sensitivity and use minimizer-based clustering anyway, since the number of more divergent alleles not identified would probably be small, as evidenced by the small differences between chewBBACA's execution modes 3 and 4 (ref image). However, since I did not test this hypothesis with datasets of other species, I decided to opt for the more conservative approach.

The limitation of minimizer-based clustering in the identification of divergent alleles may be surpassed by the implementation of spaced-minimizer- or strobemer-based clustering. Spaced-minimizers and strobemers allow for sequence variation between matching regions, which can better accommodate the differences identified in more divergent alleles. If successful, this approach could further reduce chewBBACA's runtime and improve accuracy. To further increase the accuracy of the schema creation and the allele calling, creating a graph representation of the intra- and inter-cluster connections between similar CDSs could enable the identification of connected components matching groups of similar alleles. Connected components would facilitate the identification of alleles belonging to the same locus, including divergent ones not directly linked but sharing a BSR $\geq0.6$ with other alleles belonging to the same connected component, and improve the detection of paralogous loci based on sequence similarity and genomic context analysis.

\section{Leveraging the potential of wg/cgMLST schemas and results}

Improving the accuracy of wg/cgMLST schema creation and allele calling is crucial to provide reliable results. However, assuming that minimum requirements for accuracy have been met, end-users are usually more interested in downstream analyses, such as the computation of distance matrices and \ac{MST}s to identify closely related strains, and the attribution of \ac{ST}s based on pre-defined nomenclatures. Those analyses can provide valuable information in surveillance and outbreak detection settings, and there are several tools or platforms that enable such analyses. However, \ac{wg/cgMLST} schemas and results hold the potential for much more detailed analyses. I developed chewBBACA's \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules to allow users to explore \ac{wg/cgMLST} schemas and results more comprehensively. The information and functionalities provided in the reports generated by both modules allow users to evaluate loci diversity and assess strain similarity in great detail at the dataset level, and up to the species level if analyzing wg/cgMLST schemas and datasets that capture the species diversity. Additionally, the modules can generate the reports locally, which can be shared by simply sharing the compressed report folder, enabling more scalable analyzes compared to centralized platforms with limited resources and promoting interoperability while respecting data privacy concerns since users can share their reports only with trusted parties. 
Although the reports already include functionalities for detailed \ac{wg/cgMLST} analysis, I think they can be significantly expanded. While the \textit{SchemaEvaluator} module has been used to evaluate schemas not generated with chewBBACA, such as schemas available in BIGSdb and Enterobase, this option was not extensively tested and it remains uncertain if the module can be used to evaluate any schema irrespective of provenance. The \textit{AlleleCallEvaluator} module only accepts results generated with chewBBACA, limiting its applicability. Changing both modules to make them compatible with the most widely used schema and allele call results data formats would definitely benefit most users. The allele call results can be utilized more effectively by developing new functionalities that enable the computation of loci and sample sets based on loci presence thresholds directly in the report, and identify lists of potentially spurious loci and low-quality genomes based on the percentage of missing data. The special classifications assigned by chewBBACA may correspond to variants with biological relevance, but the reports do not provide a detailed analysis of these classifications, which also tend to be treated simply as missing data. Therefore, presenting a more detailed analysis of the sequences assigned these classifications and their genomic context may provide valuable information to identify important variants and better understand genome dynamics.
There are also features that, while perhaps more challenging to implement, would extend the report's functionalities to the point where most users would not need to resort to other software for further insight. For example, determining the variable positions or \ac{SNPs} per locus would tap into the resolution level of \ac{SNP}-based approaches. Features for sample clustering and \ac{ST} assignment based on a \textit{ad hoc} or pre-defined nomenclature are commonly performed and have been requested by chewBBACA users, and could probably be incorporated into chewBBACA by integrating results generated by ReporTree \cite{mixao_reportree_2023}. The report generated by the \textit{SchemaEvaluator} module provides detailed information about the number of alleles and allele size per locus, as well as a \ac{MSA} component to compare translated alleles and identify sequence differences, but it does not provide a more general measure of similarity at the intra- and inter-locus level. For that end, representations of intra- and inter-locus similarity based on the BSR could help users in quickly identifying the loci with greater variability and groups of similar loci. The AlleleCallEvaluator module lacks a dedicated page for each sample included in the dataset being analyzed. A dedicated page per sample with summary classification statistics and a genome circular viewer, such as CGView.js\footnote{\url{https://js.cgview.ca/index.html}} \cite{stothard_visualizing_2019}, that supports the representation of annotated loci features would provide an easy way to identify loci of interest and explore the genome structure and loci genomic context.

\section{The unrealized potential of wgMLST}

I have frequently referred to \ac{wg/cgMLST}, but in reality most analyzes are performed at the cgMLST level at most. Working at the cgMLST level provides robust results for most applications, such as surveillance, and avoids having to deal with the issue of missing data when considering a greater number of loci, such as when working at the wgMLST level. When greater resolution is needed, such as in outbreak investigation, cgMLST results can be complemented with other methods, such as variant calling with a closely related reference genome. The process of creating a cgMLST schema is relatively simple, especially compared to the creation of a robust wgMLST schema, such as the one created for \textit{S. pyogenes} in Chapter \ref{ch:paper3}. For example, simply selecting a set of high-quality closed genomes for a bacterial species to create a schema with chewBBACA's \textit{CreateSchema} module, followed by allele calling with the same set of genomes and core-genome determination with the \textit{ExtractCgMLST} module, will generally yield a cgMLST schema that stabilizes quickly after allele calling with more diverse datasets and is useful for high-resolution typing in most settings. Identifying and removing spurious and paralogous loci may further increase the quality of the cgMLST schema, but these issues tend to be few and have a limited impact on the accuracy of cgMLST. Scaling to wgMLST is more complex, as it factors in the accessory genome, which may be highly diverse due to the effect of recombination and \ac{MGEs}.

Greater sequence variability may translate into a greater number of spurious loci in wgMLST schemas due to truncated alleles, gene fusions, and greater allele diversity, potentially blurring the limits of loci and allele definitions. These issues were encountered when creating the wgMLST schema for \textit{S. pyogenes}, as described in Chapter \ref{ch:paper3}. The initial wgMLST schema, containing 3,318 distinct loci, identified from a dataset of high-quality 208 complete genomes, was curated to identify and correct issues, resulting in a final schema with 3,044 loci. The reduction in the number of loci was not due to simply excluding loci from the schema. It was the result of a laborious curation process guided by an expert in \textit{S. pyogenes} to identify spurious schema loci corresponding to truncated genes, gene fusions, and paralogous loci based on the functional annotation of all loci and on the inspection of the sequences and genomic context of hundreds of schema loci. The final curation process consisted of the substitution of schema loci that matched truncated genes and gene fusions by valid alleles, merging schema loci corresponding to the same locus, and removing schema loci only when it was not possible to identify valid alternative alleles. Removing groups of paralogous loci identified by chewBBACA and loci mostly assigned special classifications is a simpler alternative to the laborious curation process used to create the \textit{S. pyogenes} wgMLST schema, but it potentially removes a lot of loci that can be corrected through a more careful curation process. A thorough curation process maximizes the quality and loci diversity accurately captured by a wgMLST schema, but the requirements for such a process are a great limitation to developing comprehensive and reliable wgMLST schemas. Moreover, some studies suggest that wgMLST does not constitute a very significant improvement in terms of resolution over cgMLST \cite{uelze_typing_2020}. So why even bother creating wgMLST schemas?

Regarding the requirements for creating wgMLST schemas, I consider that we are simply missing the tools to create high-quality wgMLST schemas. In fact, I started a project called Schema Refinery\footnote{\url{https://github.com/B-UMMI/Schema_Refinery}} with the objective of creating a set of tools to help users in common steps of wg/cgMLST schema creation. Initially, I included Python scripts to download and select genome assemblies for schema creation and allele calling, annotate schema loci based on multiple sources, and perform basic operations to refine schemas such as merging, splitting, and excluding loci. Schema Refinery has been under active development, although the bulk of the development has been passed to other members of the lab, while I participate in the discussions about implementation design, test the functionalities, and occasionally contribute with code changes. Just as in other fields, one can apply the old adage of \ac{GIGO} to wg/cgMLST. Schema Refinery enables the download and selection of high-quality genomes for schema creation and allele calling, which will greatly reduce the number of issues related to spurious loci identified in low-quality genome assemblies. Furthermore, a module to refine and expand wg/cgMLST schemas is in the final stages of development. This module will allow users to identify and resolve issues such as those identified during the creation of the wgMLST schema for \textit{S. pyogenes} automatically, and will provide the option to identify new loci from chewBBACA's allele calling results to add to the schemas. The possibility of automatically refining schemas will greatly simplify the creation of high-quality wgMLST schemas, and the option to add new loci has the potential to be a future-proof solution to update schemas as we reveal more of the diversity of bacterial species.

Concerning the observation that wgMLST may not constitute a considerable improvement over \ac{cgMLST}, I think such a conclusion can only be drawn from very rigid experimental designs. Firstly, \ac{wgMLST} schemas can contain a lot more loci than cgMLST schemas, often more than double the loci, capturing a lot more of the diversity of bacterial species. \ac{wgMLST} and cgMLST can be strongly correlated, leading to similar results and conclusions when comparing, for example, tree topologies. However, it is important to compare the resolution provided by both approaches in terms of the number of differences identified between bacterial strains. A cgMLST schema that contains half or less than the average number of loci in the genomes of a bacterial species cannot possibly provide the same resolution as a wgMLST schema that includes nearly every locus ever identified in the genomes of the same species. Thus, an apparent equivalence of both approaches is only true when the objectives of the study limit the analysis to a lower resolution level than what wgMLST can provide. A detailed analysis of the loci and population diversity of a species, such as what is performed in pangenome analysis, is only possible at the wgMLST level, as restricting the analysis to the cgMLST level would force homogeneity and under-evaluate the genetic diversity of the species. In fact, a wgMLST schema that captures a species diversity could also be called a \ac{pgMLST} schema, and could, in theory, allow for pangenome analysis with chewBBACA by analyzing a dataset representative of the diversity of the species with the \textit{AlleleCall} module and generating reports with the the \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules. It is also important to note that the wgMLST schemas contain at least part of what is considered the accessory genome. Genes determinant for virulence and antibiotic resistance are often part of the accessory genome, meaning that they may be detected by wgMLST but not by cgMLST. The increased resolution of wgMLST can be especially valuable in the context of surveillance and outbreak investigation, as demonstrated for the identification of the \textit{S. pyogenes} M1 lineages in Chapter \ref{ch:paper3}. In that case, the analysis was performed by focusing on the core genes shared by the lineages' strains, but since the schema used was a wgMLST schema it allowed to tune up the core-genome based on the dataset, which would not be possible with a more strict cgMLST schema. The strategy of using a wgMLST schema to scale the size of the core-genome based on the dataset under analysis would enrich the analysis performed for surveillance and outbreak detection, but it has not been adopted as routine. Additionally, the increased resolution resulting from the transition to wgMLST schemas would invalidate the distance thresholds for outbreak definition currently used. Thus, transitioning to routine wgMLST would require a comprehensive analysis to identify congruence points and establish equivalent values, ideally by defining a range of values instead of a single value, allowing greater flexibility \cite{mixao_multi-country_2025}.

Using wgMLST schemas to dynamically define the core-genome for each dataset represents a step up from using stricter cgMLST schemas. However, it still does not use all loci in the wgMLST schema, which can identify additional loci exclusive to strain subsets in the dataset under analysis. A complete transition to wgMLST is, in part, avoided due to the uncertainty that revolves around missing data. Missing data corresponds to loci that are not identified in the genomes, classified as absent (e.g. the \textit{LNF} classification assigned by chewBBACA), or to loci for which the validity of the match found is somewhat ambiguous (e.g., the match is considerably shorter or longer than the matched locus, corresponding to the \textit{ASM} and \textit{ALM} classifications assigned by chewBBACA, respectively). The majority of WGS data are available as sequencing reads in FASTQ format or, if assembled, as draft genome assemblies. The impossibility of having complete genomes introduces uncertainty about loci presence, as we cannot always distinguish between loci that are truly absent or appear absent due to issues introduced by sequencing and assembly errors, or simply because the best we can get out of the sequencing data is a fragmented genome assembly. The loci presence threshold used to determine the core genome from \ac{wg/cgMLST} data is usually set to 99\% or 95\%, instead of 100\%, precisely to accommodate issues such as these that cause a drop in the frequency of highly frequent loci.
The transition to wgMLST is also hindered by the methods and parameters used for loci identification. Defining simple parameters for loci identification, such as 80\% sequence identify and coverage, or a BSR $\geq0.6$ as used by chewBBACA, represents an oversimplification of locus diversity. A single or a few parameter values do not allow for perfect loci identification. Perfect loci identification, if possible at all, may require the definition of finely tuned parameter values for each locus. Some wg/cgMLST platforms, such as BIGSdb allow to define locus-specific parameter values. However, we cannot determine the optimal set of parameters for every locus. Thus, uncertainty and doubt remain, both important in science\footnote{An interesting read about this is the first lecture in "The Meaning of It All: Thoughts of a Citizen Scientist" by Richard Feynman}.

\section{Perspectives on the future of wg/cgMLST}




Sequencing data quality might soon allow to perform allele calling directly from reads more easily or obtain better genomes.

Interoperability (compatibility between platforms, especially since methods have different resolution and a detailed congruence analysis is necessary to determine if they're comparable).

Fully leveraging the potential of wg/cgMLST, by comprehensive analysis of the data, and by creating wgMLST schemas.

Approximating wg/cgMLST and other approaches, such as including SNP analysis between alleles, etc.
