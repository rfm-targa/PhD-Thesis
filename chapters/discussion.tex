\renewcommand*{\thefootnote}{\arabic{footnote}}

\mbox{}\\
\vspace{8cm}

\section{A brief note on software development practices}

I start this discussion by mentioning a subject that tends to be overlooked in bioinformatics: good practices in software development. After the initial thrill of coming up with a concept for a tool and implementing a rudimentary proof-of-concept, it is time to tidy up. Developing bioinformatics software that is efficient, maintainable, and reliable is the shortest route to contribute to its wide adoption and long-term success \cite{jimenez_four_2017, coelho_for_2024, black_ten_2020}. The process and effort of writing well-structured and documented code can take up as much time as the algorithmic intricacies, but it is essential to ensure maintainability, especially in the long-term. While I think it is not necessary to follow each piece of advice given by Uncle Bob\footnote{\url{https://en.wikipedia.org/wiki/Robert_C._Martin}}, defining a handful of simple requirements for good code quality can help avoid turning something simple into a convoluted mess.

While working on chewBBACA 3 and Chewie-NS, presented in \textbf{\autoref{ch:paper1}} and \textbf{\autoref{ch:paper2}}, I aimed to maintain good code quality standards, as I knew that that could greatly influence long-term support. In chewBBACA 3, the compartmentalization of the code into multiple modules containing smaller functions played an important role in organizing the code to maximize reusability and maintainability. When evident, functions were added to modules based on more general categories, such as functions for file operations (reading and writing files), sequence manipulation and clustering (processing and clustering of DNA and protein sequences), and multiprocessing operations (functions to divide and process inputs in parallel to improve efficiency). Compartmentalization helped to keep track of the set of functions that I had already implemented, reducing the tendency to reimplement functions in distinct parts of the code, and providing a collection of reusable functions interconnecting the code and making the codebase more coherent.

When development stretches over long periods, as was the case for chewBBACA 3, often forgetful developers (!) or new contributors will have difficulty understanding the code if it is not well documented. That is why I included \textit{docstrings} in almost every function in chewBBACA 3 and added comments to code parts whose function may seem cryptic or when it is important to explain design choices. The code documentation made it easier to jump right into development and avoid adding redundant or code-breaking changes by better understanding what the code was doing and the parts that needed to be improved or expanded. From an end-user perspective, especially for non-technical users, usage documentation is more important than code documentation. A well-structured and well-written usage documentation helps users quickly understand how to start using software and explore its functionalities \cite{karimzadeh_top_2018}. In that regard, I co-created usage documentation for chewBBACA 3 and \ac{Chewie-NS}, with quick start guides, step-by-step tutorials, and detailed instructions about each module's functionalities or for the \ac{API} endpoints and \ac{UI} in the case of \ac{Chewie-NS}. For command line software such as chewBBACA 3 it is also important to provide clear usage documentation and relevant information about each step being executed and any warnings or errors that might be raised \cite{seemann_ten_2013}. I worked on adding clear and detailed help messages for chewBBACA's modules and on printing information about normal execution or warnings and errors to the command line to help users understand each step and help resolve any issues. However, I did not add the creation of a log file, which is an excellent way to save detailed information about the execution of the process for post-analysis and debugging. 

Providing clear instructions and multiple options for installation is also of utmost importance, as difficulties during software installation often discourage users from using it \cite{alser_packaging_2024}. In this regard, making software available through a package management system, such as \textit{conda}\footnote{\url{https://anaconda.org/anaconda/conda}} (for bioinformatics software, the Bioconda distribution channel \cite{gruning_bioconda_2018} is often used) or \textit{PyPI}\footnote{\url{https://pypi.org/}} for Python packages simplifies the installation process. chewBBACA 3 is available through both \textit{conda} and \textit{PyPI}, which allows users to install it quickly and painlessly, avoiding common installation issues, such as having to resolve conflicts between incompatible dependencies versions. Another common practice in bioinformatics is the use of containerization, provided by platforms such as Docker, to package and isolate software and their dependencies, ensuring portability and consistency of the results across platforms \cite{gruening_recommendations_2018, nust_ten_2020, boettiger_introduction_2015, kadri_containers_2022}. A Docker image for chewBBACA 3 is available\footnote{\url{https://hub.docker.com/repository/docker/ummidock/chewbbaca/general}} and the deployment and management of Chewie-NS instances is only possible through the orchestration of multiple Docker containers with Docker Compose\footnote{\url{https://docs.docker.com/compose/}}. Workflow managers, such as Nextflow\footnote{\url{https://www.nextflow.io/}} and Snakemake\footnote{\url{https://snakemake.readthedocs.io/en/stable/}}, further simplify software usage by helping users set up and automate tasks \cite{wratten_reproducible_2021, strozzi_scalable_2019}. Using a workflow manager to streamline analyses with chewBBACA 3 could further simplify and promote its usage, but it is something I did not have the opportunity to work on.

Another practice that promotes reproducibility and sustainability is software testing, as regularly testing the functionality of software, especially if automated, can help assess software performance and detect unexpected issues \cite{krafczyk_scientific_2019}. Testing is a critical, but underused, aspect of scientific software development that I and others have advocated in a publication \cite{van_der_putten_software_2022}. I implemented automated standard functionality testing for chewBBACA 3 to ensure that it produces the expected results and detect issues arising due to minor changes or the introduction of new functionalities. Ultimately, I think that learning and upholding good practices for software development is crucial to developing quality software and maximizing its applicability in the long-term, as well as being a valuable skill for any developer. For Pythonistas like me, it is worth it to once in a while open a Python console and type \textit{import this}\footnote{\url{https://peps.python.org/pep-0020/}}.

\section{Limitations of CDS prediction in wg/cgMLST}

Accurate \ac{CDS} prediction is of the utmost importance in \ac{wg/cgMLST}. This is generally done in one of two ways: by aligning reference alleles to genome assemblies to identify similar alleles based on parameters such as sequence identity and coverage \cite{jolley_open-access_2018}; or through \textit{ab initio} or model-based \ac{CDS} prediction tools that scan genome assemblies to identify \ac{ORFs} that are subsequently filtered to identify putative \ac{CDSs} \cite{dimonaco_no_2022}. Prodigal is a well-known \textit{ab initio} prokaryotic \ac{CDS} predictor that chewBBACA used up to version 3.3.0 \cite{hyatt_prodigal_2010, silva_chewbbaca_2018}. However, Prodigal has not been actively maintained, raising concerns about its long-term support. As part of the work developed in \textbf{\autoref{ch:paper1}} to improve chewBBACA, I substituted Prodigal by Pyrodigal \cite{larralde_pyrodigal_2022}. Pyrodigal provides Python bindings to Prodigal, improving the performance and offering greater control over the \ac{CDS} prediction process, while also ensuring results consistent with Prodigal's latest stable version\footnote{\url{https://github.com/hyattpd/Prodigal/releases/tag/v2.6.3}}. \textbf{\autoref{ch:paper1}} evaluated the performance of chewBBACA only at the module level. Benchmarking the key steps in each of chewBBACA's modules, especially if varying the number of \ac{CPU} cores used to better assess scalability, would have provided valuable information to understand which steps were most optimized and which still have room for improvement. Although the performance of the \ac{CDS} prediction step was not directly measured, it is safe to assume that Pyrodigal, which was optimized to reduce memory usage and runtime compared to Prodigal \cite{larralde_pyrodigal_2022}, contributed to the efficiency of chewBBACA 3 compared to its previous versions.

Although efficiency gains are definitely important, the true value of switching to Pyrodigal lies in having greater control over the \ac{CDS} prediction process. With Prodigal, \ac{CDS} prediction was only possible by calling Prodigal directly. In contrast, Pyrodigal provides Python bindings that enable finer control of the \ac{CDS} prediction parameters. This allowed to seamlessly integrate \ac{CDS} prediction into chewBBACA 3 and maintain results consistent with previous chewBBACA versions. Furthermore, this can be further explored to implement changes that may significantly improve the \ac{CDS} prediction process compared to what is presented in \textbf{\autoref{ch:paper1}}. Possible improvements include selecting from multiple file formats for the output file containing the \ac{CDS} prediction results, such as GenBank and translated \ac{CDSs}, which currently can only be written in FASTA format. In addition, the scores attributed to each \ac{CDS} can be accessed before writing the results, which, through a thorough validation process, may allow the definition of a score threshold used to exclude spurious \ac{CDSs} predicted by Pyrodigal, leading to improvements in allele calling and a reduction in the number of spurious loci included in the schema seeds created by the \textit{CreateSchema} module.

The prediction of spurious \ac{CDSs} is the main disadvantage of using a \textit{ab initio} or model-based \ac{CDS} predictor. The predicted \ac{CDSs} may not match any known gene, making validation difficult, or there may be a consistent deviation from reference alleles that have been experimentally validated, such as when the predicted \ac{CDSs} are in-frame but shorter than the reference alleles or are out of frame compared to reference alleles \cite{dimonaco_no_2022}. This may contrast with alignment-based \ac{CDS} prediction, used in \ac{wg/cgMLST} platforms such as \ac{BIGSdb}, which may predict less spurious \ac{CDSs} due to the use of more conservative parameters, but will have a limited capacity to identify more divergent alleles or \ac{CDSs} corresponding to genes not in the reference database. The discrepancies between both \ac{CDS} prediction strategies hinder reproducibility. wg/cgMLST schemas and results generated with chewBBACA 3 will not be directly comparable and may lead to different conclusions than the results generated on a platform that uses an alignment-based \ac{CDS} predictor. Adding alignment-based CDS prediction to chewBBACA 3 can help complement Pyrodigal results, while also allowing users to define parameters that approximate those used by other \ac{wg/cgMLST} platforms, promoting interoperability and further adoption of chewBBACA 3. In fact, a combinatory approach is already used in some genome annotation pipelines, such as Prokka, Bakta, and PGAP, to improve \ac{CDS} prediction and annotation \cite{seemann_prokka_2014, schwengers_bakta_2021, li_refseq_2021}. Adopting a similar strategy in chewBBACA 3, especially if also defining a threshold based on the scores computed by Pyrodigal to filter out potential spurious \ac{CDSs}, would definitely improve \ac{CDS} prediction and, consequently, the accuracy of the results. Validating the results produced by this strategy would require a thorough evaluation with reference datasets for both experimentally validated and spurious \ac{CDSs}, which could prove to be a laborious process, but would certainly help establish a framework for more accurate \ac{CDS} prediction, especially in \ac{wg/cgMLST}.

\section{Advantages and disadvantages of hash-based allele identification}

The high \ac{ANI} values between the genomes of the species datasets used to evaluate chewBBACA 3 in \textbf{Chapter \ref{ch:paper1}} reflect the composition of the public databases from where the genomes were sourced \cite{oleary_reference_2016, blackwell_exploring_2021}. Through the analysis of datasets used in other studies and testing datasets created to evaluate chewBBACA's performance, I observed that, for most species, the degree of inter-species similarity in available genome collections was also high, on average. This observation is supported by multiple studies that indicate \% ANI as the average for species-level...(CITE SOME ANI PAPER!). Genomes with highly similar sequences may share a significant fraction of identical \ac{CDSs}. Thus, using alignment to compare loci in a wg/cgMLST schema with genomes in a pairwise fashion can be inefficient and redundant if the process keeps identifying the same \ac{CDSs}.

The first published version of chewBBACA already determined if the \ac{CDSs} predicted from the input genomes were in the schema without using alignment, but would do this for every single genome, even if the same \ac{CDS} was found in all genomes. To further optimize the exact matching process, I decided to implement a deduplication step after the gene prediction step. This deduplication step identifies the set of distinct \ac{CDSs} identified in all input genomes, returning a hash table with \ac{CDS} SHA-256 hashes as keys and the list of genomes containing each \ac{CDS} as values. The SHA-256 hashes are used as unique identifiers for the exact matching between the \ac{CDSs} and the schema allele hashes, and are shorter than the actual \ac{CDS} sequences, which reduces memory usage compared to using the sequences as keys. Associating the list of genomes with a \ac{CDS} to the \ac{CDS} SHA-256 hash allows identifying and classifying all genomes with a particular \ac{CDS}, avoiding redundant comparisons. An issue that I identified while testing \ac{CDS} deduplication with large datasets was that the lists of genome string identifiers associated to each key could contain many values and occupy a lot of memory, which meant that storing the hash table in memory could become impractical for systems without a lot of \ac{RAM}. Mapping the genome string identifiers to unique integer identifiers and using lists of integer identifiers as values reduced memory usage but was not enough to drop memory usage to the range of values adequate for most users to be able to perform large-scale analysis. To further reduce memory usage, I used modified polyline encoding (CITE NUMCOMPRESS) to compress the lists of genome integer identifiers, yielding strings that occupy less memory than the original lists, and allowing large-scale analysis with the memory available in most modern laptops, as reported in Chapter \ref{ch:paper1} and testing during development.

Although implementing a solution for exact matching that integrates sequence deduplication, sequence hashing, and modified polyline encoding is more complex than simply comparing the CDSs identified in each genome against a \ac{wg/cgMLST} schema, the gains in efficiency far outweigh the increase in complexity since it allows chewBBACA to be used at a scale that was not previously possible.

One possible limitation of this approach is that it assumes that the CDSs identified in larger genome collections are highly repeated. This is in part true for single-species datasets. If it is applied at higher taxonomic ranks, such as at the genus level, sequence diversity may be considerably higher, increasing the size of the hash table and, consequently, memory usage. Performance at the genus level has not been tested in the results presented in this thesis, but there have been applications of wg/cgMLST at that level, such as evidenced by the \textit{Brucella} and \textit{Shewanella} schemas available on Chewie-NS, suggesting that there have been successful applications at the genus level. Applications above the genus level are not envisioned, mainly because the degree of sequence divergence between taxa at higher taxonomic levels may compromise the operational definition of loci used in wg/cgMLST. Another possible limitation is that sequencing tends to be used in studies or interventions deemed more relevant, such as for the sequencing of bacterial species with a significant clinical or economical impact, leading to an incomplete representation of the diversity of bacterial species. In the event that future sequencing experiments begin to reveal more of the diversity of bacterial species, an increase in sequence diversity would also result in an increase in memory usage.

It is important to note that any concerns about increased memory usage may be minimized in the near future as the computational resources available to the general public become more and more powerful.

\section{Classification of novel alleles}

The most common way of identifying loci alleles in bacterial strains is perhaps by aligning a representative allele against assembled genomes. This is usually performed with a sequence aligner, such as \ac{BLAST} \cite{camacho_blast_2009}, and by defining identity and coverage thresholds for the identification of \textit{bona fide} alleles. Although alignment-based approaches work for most cases, they may have some limitations, especially when comparing alleles with significant sequence divergence, either due to accumulation of point mutations and small indels or caused by recombination or \ac{MGEs}. For instance, the extension of local alignments may halt due to significant sequence divergence, yielding local alignments that, while having a high score, do not meet the identity and coverage criteria enforced by the allele callers. This may result in the inability to identify loci that are, in fact, present in genomes. In the specific case of \ac{BLASTp}, divergence in a single sequence region can halt alignment extension between alleles that are otherwise highly similar. In these cases, \ac{BLASTp} can return single or multiple local alignments between alleles that separately, and in the case of chewBBACA, do not reach the \ac{BSR} threshold or would have to be combined to determine if the \ac{BSR} computed from the combined raw alignment scores is sufficient. BLAST is used to determine local alignments, so it seems reasonable that it does not provide any option to combine multiple local alignments to get a combined score. Initially, and perhaps naively, I thought it may be possible to implement a simple script to compute the combined score. But the lack of knowledge about how the alignment raw score is computed, especially with compositional adjustment, made it seem like I was trying to probe a black box. Thus, it appeared reasonable to consider other options. At first glance, one of the options could be to compute global alignments. Although global alignments would cover the full length of the sequences being compared, two factors contributed to ruling it out nearly immediately.

Firstly, global alignment yields different results than local alignment and I would have to use a different aligner than BLAST. This would lead to completely different results, invalidating the BSR-based approach used in chewBBACA, and potentially diverge greatly from the results determined with BLAST, which would warrant a complete overhaul of chewBBACA's algorithm, both conceptually and algorithmically. Secondly, global alignment is computationally expensive and is not adequate to align large collections of potentially divergent sequences. Since I intended to implement a new version of chewBBACA tailored for scalable and efficient analysis of large genome collections, I decided to complement the alignment-based strategy used by chewBBACA with alignment-free methods, more specifically, k-mer-based methods.

The concept of k-mers, although seemingly simple, offers extreme versatility by simply varying the value of \textit{k}, which defines the size of the k-mers, and choosing a sampling method to select subsets of k-mers from a sequence. Research into the optimization of k-mer-based methods has produced concepts such as spaced seeds, minimizers, syncmers, and, more recently, strobemers. These concepts are applicable to virtually any step of the bioinformatics workflow, contributing to the efficiency and accuracy of methods for read mapping, sequence alignment, taxonomic classification, and metagenomic binning, among others.

I chose minimizer-based clustering to complement the alignment-based strategy used by chewBBACA because minimizers are simple to implement, fast to determine, and enabled me to implement a clustering method that was accurate enough for the application I had in mind. Performing intra-cluster alignment to classify the CDSs is much faster than all-vs-all or pairwise alignment between the CDSs identified in all inputs. Minimizer-based clustering also enables the identification of highly similar sequences with differences concentrated in a single or a few sequence regions because it counts the number of shared minimizers along the complete sequences, whereas BLAST may only report smaller local alignments that make it more difficult to determine global sequence identity. The strength of the strategy lies then in combining alignment-based and alignment-free methods to improve speed and accuracy, an approach that has been increasingly used in bioinformatics. Although minimizer-based clustering enabled considerably faster and slightly more accurate wg/cgMLST with chewBBACA, it is not sufficiently sensitive to identify some of the most divergent alleles within the $0.6\geq BSR > 0.7$ interval. Due to this limitation, the classification of the most divergent alleles in chewBBACA 3 uses only BLASTp, which can increase the runtime with larger and more diverse datasets. An alternative approach would be to accept a slight reduction in sensitivity and use minimizer-based clustering anyway, since the number of more divergent alleles not identified would probably be small, as evidenced by the small differences between chewBBACA's execution modes 3 and 4 (ref image). However, since I did not test this hypothesis with datasets of other species, I decided to opt for the more conservative approach.

The limitation of minimizer-based clustering in the identification of divergent alleles may be surpassed by the implementation of spaced-minimizer- or strobemer-based clustering. Spaced-minimizers and strobemers allow for sequence variation between matching regions, which can better accommodate the differences identified in more divergent alleles. If successful, this approach could further reduce chewBBACA's runtime and improve accuracy. To further increase the accuracy of the schema creation and the allele calling, creating a graph representation of the intra- and inter-cluster connections between similar CDSs could enable the identification of connected components matching groups of similar alleles. Connected components would facilitate the identification of alleles belonging to the same locus, including divergent ones not directly linked but sharing a BSR $\geq0.6$ with other alleles belonging to the same connected component, and improve the detection of paralogous loci based on sequence similarity and genomic context analysis.

\section{Leveraging the potential of wg/cgMLST schemas and results}

Improving the accuracy of wg/cgMLST schema creation and allele calling is crucial to provide reliable results. However, assuming that minimum requirements for accuracy have been met, end-users are usually more interested in downstream analyses, such as the computation of distance matrices and \ac{MST}s to identify closely related strains, and the attribution of \ac{ST}s based on pre-defined nomenclatures. Those analyses can provide valuable information in surveillance and outbreak detection settings, and there are several tools or platforms that enable such analyses. However, \ac{wg/cgMLST} schemas and results hold the potential for much more detailed analyses. I developed chewBBACA's \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules to allow users to explore \ac{wg/cgMLST} schemas and results more comprehensively. The information and functionalities provided in the reports generated by both modules allow users to evaluate loci diversity and assess strain similarity in great detail at the dataset level, and up to the species level if analyzing wg/cgMLST schemas and datasets that capture the species diversity. Additionally, the modules can generate the reports locally, which can be shared by simply sharing the compressed report folder, enabling more scalable analyzes compared to centralized platforms with limited resources and promoting interoperability while respecting data privacy concerns since users can share their reports only with trusted parties. 
Although the reports already include functionalities for detailed \ac{wg/cgMLST} analysis, I think they can be significantly expanded. While the \textit{SchemaEvaluator} module has been used to evaluate schemas not generated with chewBBACA, such as schemas available in BIGSdb and Enterobase, this option was not extensively tested and it remains uncertain if the module can be used to evaluate any schema irrespective of provenance. The \textit{AlleleCallEvaluator} module only accepts results generated with chewBBACA, limiting its applicability. Changing both modules to make them compatible with the most widely used schema and allele call results data formats would definitely benefit most users. The allele call results can be utilized more effectively by developing new functionalities that enable the computation of loci and sample sets based on loci presence thresholds directly in the report, and identify lists of potentially spurious loci and low-quality genomes based on the percentage of missing data. The special classifications assigned by chewBBACA may correspond to variants with biological relevance, but the reports do not provide a detailed analysis of these classifications, which also tend to be treated simply as missing data. Therefore, presenting a more detailed analysis of the sequences assigned these classifications and their genomic context may provide valuable information to identify important variants and better understand genome dynamics.
There are also features that, while perhaps more challenging to implement, would extend the report's functionalities to the point where most users would not need to resort to other software for further insight. For example, determining the variable positions or \ac{SNPs} per locus would tap into the resolution level of \ac{SNP}-based approaches. Features for sample clustering and \ac{ST} assignment based on a \textit{ad hoc} or pre-defined nomenclature are commonly performed and have been requested by chewBBACA users, and could probably be incorporated into chewBBACA by integrating results generated by ReporTree \cite{mixao_reportree_2023}. The report generated by the \textit{SchemaEvaluator} module provides detailed information about the number of alleles and allele size per locus, as well as a \ac{MSA} component to compare translated alleles and identify sequence differences, but it does not provide a more general measure of similarity at the intra- and inter-locus level. For that end, representations of intra- and inter-locus similarity based on the BSR could help users in quickly identifying the loci with greater variability and groups of similar loci. The AlleleCallEvaluator module lacks a dedicated page for each sample included in the dataset being analyzed. A dedicated page per sample with summary classification statistics and a genome circular viewer, such as CGView.js\footnote{\url{https://js.cgview.ca/index.html}} \cite{stothard_visualizing_2019}, that supports the representation of annotated loci features would provide an easy way to identify loci of interest and explore the genome structure and loci genomic context.

\section{The unrealized potential of wgMLST}

I have frequently referred to \ac{wg/cgMLST}, but in reality most analyzes are performed at the cgMLST level at most. Working at the cgMLST level provides robust results for most applications, such as surveillance, and avoids having to deal with the issue of missing data when considering a greater number of loci, such as when working at the wgMLST level. When greater resolution is needed, such as in outbreak investigation, cgMLST results can be complemented with other methods, such as variant calling with a closely related reference genome. The process of creating a cgMLST schema is relatively simple, especially compared to the creation of a robust wgMLST schema, such as the one created for \textit{S. pyogenes} in Chapter \ref{ch:paper3}. For example, simply selecting a set of high-quality closed genomes for a bacterial species to create a schema with chewBBACA's \textit{CreateSchema} module, followed by allele calling with the same set of genomes and core-genome determination with the \textit{ExtractCgMLST} module, will generally yield a cgMLST schema that stabilizes quickly after allele calling with more diverse datasets and is useful for high-resolution typing in most settings. Identifying and removing spurious and paralogous loci may further increase the quality of the cgMLST schema, but these issues tend to be few and have a limited impact on the accuracy of cgMLST. Scaling to wgMLST is more complex, as it factors in the accessory genome, which may be highly diverse due to the effect of recombination and \ac{MGEs}.

Greater sequence variability may translate into a greater number of spurious loci in wgMLST schemas due to truncated alleles, gene fusions, and greater allele diversity, potentially blurring the limits of loci and allele definitions. These issues were encountered when creating the wgMLST schema for \textit{S. pyogenes}, as described in Chapter \ref{ch:paper3}. The initial wgMLST schema, containing 3,318 distinct loci, identified from a dataset of high-quality 208 complete genomes, was curated to identify and correct issues, resulting in a final schema with 3,044 loci. The reduction in the number of loci was not due to simply excluding loci from the schema. It was the result of a laborious curation process guided by an expert in \textit{S. pyogenes} to identify spurious schema loci corresponding to truncated genes, gene fusions, and paralogous loci based on the functional annotation of all loci and on the inspection of the sequences and genomic context of hundreds of schema loci. The final curation process consisted of the substitution of schema loci that matched truncated genes and gene fusions by valid alleles, merging schema loci corresponding to the same locus, and removing schema loci only when it was not possible to identify valid alternative alleles. Removing groups of paralogous loci identified by chewBBACA and loci mostly assigned special classifications is a simpler alternative to the laborious curation process used to create the \textit{S. pyogenes} wgMLST schema, but it potentially removes a lot of loci that can be corrected through a more careful curation process. A thorough curation process maximizes the quality and loci diversity accurately captured by a wgMLST schema, but the requirements for such a process are a great limitation to developing comprehensive and reliable wgMLST schemas. Moreover, some studies suggest that wgMLST does not constitute a very significant improvement in terms of resolution over cgMLST \cite{uelze_typing_2020}. So why even bother creating wgMLST schemas?

Regarding the requirements for creating wgMLST schemas, I consider that we are simply missing the tools to create high-quality wgMLST schemas. In fact, I started a project called Schema Refinery\footnote{\url{https://github.com/B-UMMI/Schema_Refinery}} with the objective of creating a set of tools to help users in common steps of wg/cgMLST schema creation. Initially, I included Python scripts to download and select genome assemblies for schema creation and allele calling, annotate schema loci based on multiple sources, and perform basic operations to refine schemas such as merging, splitting, and excluding loci. Schema Refinery has been under active development, although the bulk of the development has been passed to other members of the lab, while I participate in the discussions about implementation design, test the functionalities, and occasionally contribute with code changes. Just as in other fields, one can apply the old adage of \ac{GIGO} to wg/cgMLST. Schema Refinery enables the download and selection of high-quality genomes for schema creation and allele calling, which will greatly reduce the number of issues related to spurious loci identified in low-quality genome assemblies. Furthermore, a module to refine and expand wg/cgMLST schemas is in the final stages of development. This module will allow users to identify and resolve issues such as those identified during the creation of the wgMLST schema for \textit{S. pyogenes} automatically, and will provide the option to identify new loci from chewBBACA's allele calling results to add to the schemas. The possibility of automatically refining schemas will greatly simplify the creation of high-quality wgMLST schemas, and the option to add new loci has the potential to be a future-proof solution to update schemas as we reveal more of the diversity of bacterial species.

Concerning the observation that wgMLST may not constitute a considerable improvement over \ac{cgMLST}, I think such a conclusion can only be drawn from very rigid experimental designs. Firstly, \ac{wgMLST} schemas can contain a lot more loci than cgMLST schemas, often more than double the loci, capturing a lot more of the diversity of bacterial species. \ac{wgMLST} and cgMLST can be strongly correlated, leading to similar results and conclusions when comparing, for example, tree topologies. However, it is important to compare the resolution provided by both approaches in terms of the number of differences identified between bacterial strains. A cgMLST schema that contains half or less than the average number of loci in the genomes of a bacterial species cannot possibly provide the same resolution as a wgMLST schema that includes nearly every locus ever identified in the genomes of the same species. Thus, an apparent equivalence of both approaches is only true when the objectives of the study limit the analysis to a lower resolution level than what wgMLST can provide. A detailed analysis of the loci and population diversity of a species, such as what is performed in pangenome analysis, is only possible at the wgMLST level, as restricting the analysis to the cgMLST level would force homogeneity and under-evaluate the genetic diversity of the species. In fact, a wgMLST schema that captures a species diversity could also be called a \ac{pgMLST} schema, and could, in theory, allow for pangenome analysis with chewBBACA by analyzing a dataset representative of the diversity of the species with the \textit{AlleleCall} module and generating reports with the the \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules. It is also important to note that the wgMLST schemas contain at least part of what is considered the accessory genome. Genes determinant for virulence and antibiotic resistance are often part of the accessory genome, meaning that they may be detected by wgMLST but not by cgMLST. The increased resolution of wgMLST can be especially valuable in the context of surveillance and outbreak investigation, as demonstrated for the identification of the \textit{S. pyogenes} M1 lineages in Chapter \ref{ch:paper3}. In that case, the analysis was performed by focusing on the core genes shared by the lineages' strains, but since the schema used was a wgMLST schema it allowed to tune up the core-genome based on the dataset, which would not be possible with a more strict cgMLST schema. The strategy of using a wgMLST schema to scale the size of the core-genome based on the dataset under analysis would enrich the analysis performed for surveillance and outbreak detection, but it has not been adopted as routine. Additionally, the increased resolution resulting from the transition to wgMLST schemas would invalidate the distance thresholds for outbreak definition currently used. Thus, transitioning to routine wgMLST would require a comprehensive analysis to identify congruence points and establish equivalent values, ideally by defining a range of values instead of a single value, allowing greater flexibility \cite{mixao_multi-country_2025}.

Using wgMLST schemas to dynamically define the core-genome for each dataset represents a step up from using stricter cgMLST schemas. However, it still does not use all loci in the wgMLST schema, which can identify additional loci exclusive to strain subsets in the dataset under analysis. A complete transition to wgMLST is, in part, avoided due to the uncertainty that revolves around missing data. Missing data corresponds to loci that are not identified in the genomes, classified as absent (e.g. the \textit{LNF} classification assigned by chewBBACA), or to loci for which the validity of the match found is somewhat ambiguous (e.g., the match is considerably shorter or longer than the matched locus, corresponding to the \textit{ASM} and \textit{ALM} classifications assigned by chewBBACA, respectively). The majority of WGS data are available as sequencing reads in FASTQ format or, if assembled, as draft genome assemblies. The impossibility of having complete genomes introduces uncertainty about loci presence, as we cannot always distinguish between loci that are truly absent or appear absent due to issues introduced by sequencing and assembly errors, or simply because the best we can get out of the sequencing data is a fragmented genome assembly. The loci presence threshold used to determine the core genome from \ac{wg/cgMLST} data is usually set to 99\% or 95\%, instead of 100\%, precisely to accommodate issues such as these that cause a drop in the frequency of highly frequent loci.
The transition to wgMLST is also hindered by the methods and parameters used for loci identification. Defining simple parameters for loci identification, such as 80\% sequence identify and coverage, or a BSR $\geq0.6$ as used by chewBBACA, represents an oversimplification of locus diversity. A single or a few parameter values do not allow for perfect loci identification. Perfect loci identification, if possible at all, may require the definition of finely tuned parameter values for each locus. Some wg/cgMLST platforms, such as BIGSdb allow to define locus-specific parameter values. However, we cannot determine the optimal set of parameters for every locus. Thus, I think that in many cases we will have to stick to generalizations. Uncertainty and doubt remain, both important in science\footnote{An interesting read about this is the first lecture in "The Meaning of It All: Thoughts of a Citizen Scientist" by Richard Feynman}.

\section{Perspectives on the future of wg/cgMLST}

Bioinformatics, as an interdisciplinary field, has greatly benefited from the application of concepts and methods derived from other scientific fields. The achievement of technological milestones and growing interest are propelling bioinformatics into a flourishing age and to the center stage of scientific research. The performance of computing hardware has been steadily improving and has reached a point where it is possible to generate, store, and analyze huge amounts of biological data. More widespread access to powerful computational resources allows more researchers to apply existing or newly developed methods to tackle challenging problems that were once off limits due to technological and technical constraints. Several public health emergencies, such as the COVID-19 pandemic, caused by the SARS-CoV-2 virus, and mortality caused by major bacterial pathogens, especially in low- and middle-income countries, have also raised the public and governmental organizations' awareness of the importance of research and preventive measures.

The boom in GPU computing, in conjunction with the rising interest and advances in AI, will definitely continue to accelerate biomedical research and expand the realm of research possibilities. A good example of a recent and monumental breakthrough due to the application of GPU computing and AI is AlphaFold (cite), which has greatly improved our ability to accurately predict protein structures. The true impact of AlphaFold will probably only be fully realized in the coming years. AI has already been applied to prokaryotic gene prediction \cite{sommer_balrog_2021}, a crucial aspect for accurate wg/cgMLST. A more accurate prediction of protein structure and function can also help validate gene prediction data, contributing to improvements in gene prediction and wg/cgMLST schema refinement.

When talking about the future of wg/cgMLST, one cannot simply forget to mention the relatively recent improvements to DNA sequencing technologies that have enabled WGS and consequently wg/cgMLST. Further improvements to these technologies are expected. Any improvements, especially to the accuracy of long-read sequencing, will increase the contiguity of assembled genomes, and perhaps even make accurate wg/cgMLST from sequencing reads a reality. More contiguous genome assemblies should also lead to more accurate gene prediction. The quality of a genome assembly is influenced by multiple factors, including the methods used for sample preparation, the sequencing technology used, and the tools used to process the sequencing data and generate a genome assembly. Each step of the process and the combination of methods used during the whole process can introduce biases that negatively influence the end result. These issues contribute differently to the degree of fragmentation of genome assemblies, often leading to a cumulative effect that affects a greater number of loci as more genomes are analyzed. In my view, improvements to DNA sequencing technologies and the standardization of other steps can lead to much more than a slight increase in the accuracy of wg/cgMLST. It can greatly reduce the number of absent or fragmented genes, facilitating the transition to wgMLST, and possibly revealing that we need to rethink the definitions of core and accessory genome for many bacterial species.

I also consider that we are nearing a crossroads where we will have to rethink, or at least adapt, the methods used for wg/cgMLST. The web platforms and other software for wg/cgMLST are responsible and are also a consequence of the success of wg/cgMLST. wg/cgMLST allows for a comprehensive analysis of loci diversity and high-resolution typing in surveillance and outbreak scenarios, but even at its highest potential, it may lack some of the features of SNP-based and k-mer-based methods. In fact, wg/cgMLST results are often complemented by SNP analysis for finer resolution, such as in outbreak detection. The integration of k-mer-based methods can also increase the accuracy and speed of the analyzes, as achieved with chewBBACA 3 in Chapter \ref{ch:paper1}. I think that these different approaches will be increasingly combined to take advantage of the strengths of each one, with discussions such as allele-based vs. SNP-based becoming increasingly less relevant and priorities shifting towards the implementation of software that performs comprehensive and multifaceted analyses. In addition, I think graph-based methods can emerge as an alternative that surpasses the accuracy of other approaches. Improvements in computing hardware and graph algorithms, as well as the availability of a large number of sequenced genomes, have made it possible to index and examine the variation of larger genome collections \cite{holley_bifrost_2020, harling-lee_graph-based_2022, noll_pangraph_2023}. Pangenome graphs have been used to index and examine the variability of bacterial genomes, allowing the identification of population-level nucleotide and structural polymorphisms. Using graph data structures to index the variability of coding and non-coding regions of large collections of bacterial genomes may provide a framework that achieves results more accurate than the combination of wg/cgMLST and SNP-based approaches by allowing the analysis of non-coding regions, which cannot be done with wg/cgMLST, and surpassing the reference bias issue of SNP-based methods. All while also providing information about genome structure, which would allow to obtain information about genomic context through synteny analysis. Notwithstanding the potential of graph-based methods, the computational requirements for the construction of graphs that encompass the diversity of thousands of bacterial strains may be prohibitive for most users, hindering the applicability of graph-based methods when compared to wg/cgMLST.

The comparability of the wg/cgMLST results at the national and international levels is essential for effective surveillance and outbreak detection. During the COVID-19 pandemic, the scientific community quickly adapted and established data analysis and sharing standards to promote easy sharing and comparability of results to track SARS-CoV-2 variants (cite something, PHAGE paper). In contrast, the stage of implementation of WGS-based surveillance systems for bacterial pathogens, such as \ac{FWD} pathogens, can differ significantly between countries, with many countries investing in different methodologies. The disparities between the surveillance systems implemented by different countries hinder the comparability of the results at the intersectoral and international levels, which affects outbreak detection and investigation, especially for multi-country outbreaks. A detailed congruence analysis of the results generated by eleven European institutes revealed that, while there may be general concordance between the results generated by similar surveillance systems, the results are not directly comparable \cite{mixao_multi-country_2025}. Different surveillance systems provide different levels of resolution, and a congruence analysis is necessary to establish threshold equivalence between systems. Since the implementation of a surveillance system represents a significant investment, I consider it highly unlikely that any country will switch to a different approach in the short term, even if that would promote interoperability and provide more accurate results. A more immediate solution would be to determine congruence points between all the different systems and using flexible thresholds to accommodate for incompatibilities arising due to the use of single value thresholds. However, this approach would require a significant collaboration effort between institutions of many different countries and the validity of the congruence system would have to be continuously assessed. In the long term, the systems used by different countries may converge toward methods that are provably better, promoting interoperability and approximating a global One Health surveillance system. In the meantime, I think countries will continue sharing data on an \textit{ad hoc} basis or through centralized systems such as \ac{EFSA}'s One Health WGS system when a more concerted action is necessary to track and resolve multi-country outbreaks.

It is clear that the potential of the wg/cgMLST approach has not yet been fully explored, with room for improvement both conceptually and technically. The ongoing technological revolution also expands the realm of possibilities, encouraging researchers to apply methodologies that were once unfeasible from a technological standpoint or come up with revolutionary strategies arising from a shift in perspective. The adoption and evolution of wg/cgMLST is influenced by multiple factors that go beyond scientific objectiveness, making it difficult to predict the magnitude of its future role. However, future bacterial surveillance systems and population diversity studies, even if methodologically distinct from wg/cgMLST, will surely have to incorporate the advantageous properties of wg/cgMLST that distinguish it from other approaches currently used. The current scientific revolution will surely continue to provide a multitude of possibilities and opportunities that will approximate and blur the boundaries between all scientific fields. The uncertainty of how it will all unfold is undoubtedly exciting.
