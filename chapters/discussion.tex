\renewcommand*{\thefootnote}{\arabic{footnote}}

\mbox{}\\
\vspace{8cm}

\section{A brief note on software development practices}

I start this discussion by mentioning a subject that tends to be overlooked in bioinformatics: good practices in software development. After the initial thrill of coming up with a concept for a tool and implementing a rudimentary proof-of-concept, it is time to tidy up. Developing bioinformatics software that is efficient, maintainable, and reliable is the shortest route to contribute to its wide adoption and long-term success \cite{jimenez_four_2017, coelho_for_2024, black_ten_2020}. The process and effort of writing well-structured and documented code can take up as much time as the algorithmic intricacies, but it is essential to ensure maintainability. While I think it is not necessary to follow each piece of advice given by Uncle Bob\footnote{\url{https://en.wikipedia.org/wiki/Robert_C._Martin}}, defining a handful of simple requirements and practices for sustained software development over time can maximize software applicability and help avoid turning something simple into a convoluted mess \cite{gardner_sustained_2022}.

While working on chewBBACA 3 and \ac{Chewie-NS}, presented in \textbf{\autoref{ch:paper1}} and \textbf{\autoref{ch:paper2}}, I aimed to maintain good code quality standards, as I knew that this could greatly influence long-term support. In chewBBACA 3, the compartmentalization of the code into multiple modules containing smaller functions played an important role in organizing the code to maximize reusability and maintainability. When evident, functions were grouped into modules based on more general categories, such as functions for file operations (reading and writing files), sequence manipulation and clustering (processing and clustering of \ac{DNA} and protein sequences), and multiprocessing operations (functions to divide and process inputs in parallel to improve efficiency). Compartmentalization helped to keep track of the set of functions that I had already implemented, reducing the tendency to reimplement functions in distinct parts of the code, and providing a collection of reusable functions interconnecting the code and making the codebase more coherent.

When development stretches over long periods, as was the case for chewBBACA 3, often forgetful developers (!) or new contributors will have difficulty understanding the code if it is not well documented. That is why I included \textit{docstrings} in almost every function in chewBBACA 3 and added comments to code parts whose function may seem cryptic or when it is important to explain design choices. The code documentation made it easier to jump right into development and avoid adding redundant or code-breaking changes by better understanding what the code was doing and the parts that needed to be improved or expanded. From an end-user perspective, especially for non-technical users, usage documentation is more important than code documentation. A well-structured and well-written usage documentation helps users quickly understand how to start using software and explore its functionalities \cite{karimzadeh_top_2018}. In that regard, I co-created usage documentation for chewBBACA 3 and \ac{Chewie-NS}, with quick start guides, step-by-step tutorials, and detailed instructions about each module's functionalities or for the \ac{API} endpoints and \ac{UI} in the case of \ac{Chewie-NS}. For command line software such as chewBBACA 3 it is also important to provide clear usage documentation and relevant information about each step being executed and any warnings or errors that might be raised \cite{seemann_ten_2013}. I worked on adding clear and detailed help messages for chewBBACA's modules and on printing information about normal execution or warnings and errors to the command line to help users understand each step and help resolve any issues. However, I did not add the creation of a log file, which is an excellent way to save detailed information about the execution of the process for post-analysis and debugging. Implementing a log file should be a top priority for future versions of chewBBACA, and it is something that can be easily done using the built-in \textit{logging}\footnote{\url{https://docs.python.org/3/library/logging.html}} module.

Providing clear instructions and multiple options for installation is also of utmost importance, as difficulties during software installation often discourage users from using it \cite{alser_packaging_2024}. In this regard, making software available through a package management system, such as \textit{conda}\footnote{\url{https://anaconda.org/anaconda/conda}} (for bioinformatics software, the Bioconda distribution channel \cite{gruning_bioconda_2018} is often used) or \textit{PyPI}\footnote{\url{https://pypi.org/}} for Python packages simplifies the installation process. chewBBACA 3 is available through both \textit{conda} and \textit{PyPI}, which allows users to install it quickly and painlessly, avoiding common installation issues, such as having to resolve conflicts between incompatible dependencies versions. Another common practice in bioinformatics is the use of containerization, provided by platforms such as Docker, to package and isolate software and their dependencies, ensuring portability and consistency of the results across platforms \cite{gruening_recommendations_2018, nust_ten_2020, boettiger_introduction_2015, kadri_containers_2022}. A Docker image for chewBBACA 3 is available\footnote{\url{https://hub.docker.com/repository/docker/ummidock/chewbbaca/general}} and the deployment and management of \ac{Chewie-NS} instances is only possible through the orchestration of multiple Docker containers with Docker Compose\footnote{\url{https://docs.docker.com/compose/}}. Workflow managers, such as Nextflow\footnote{\url{https://www.nextflow.io/}} and Snakemake\footnote{\url{https://snakemake.readthedocs.io/en/stable/}}, further simplify software usage by helping users set up and automate tasks \cite{wratten_reproducible_2021, strozzi_scalable_2019}. Using a workflow manager to streamline analyses with chewBBACA 3 could further simplify and promote its usage, and while it is something I did not have the opportunity to work on, it should be a future priority.

Another practice that promotes reproducibility and sustainability is software testing, as regularly testing the functionality of software, especially if automated, can help assess software performance and detect unexpected issues \cite{krafczyk_scientific_2019}. Testing is a critical, but underused, aspect of scientific software development that I and others have advocated in a publication \cite{van_der_putten_software_2022}. I implemented automated standard functionality testing for chewBBACA 3 to ensure that it produces the expected results and detect issues arising due to minor changes or the introduction of new functionalities.

Ultimately, I think that learning and upholding good practices for sustained software development is crucial to developing quality software and maximizing its applicability in the long-term, as well as being a valuable skill for any developer. For Pythonistas like me, it is worth it to once in a while open a Python console and type \textit{import this}\footnote{\url{https://peps.python.org/pep-0020/}}.

\section{Limitations and further improvements to CDS prediction in wg/cgMLST}

Accurate \ac{CDS} prediction is of the utmost importance in \ac{wg/cgMLST}. This is generally done in one of two ways: by aligning reference alleles to genome assemblies to identify similar alleles based on parameters such as sequence identity and coverage \cite{jolley_open-access_2018}; or through \textit{ab initio} or model-based \ac{CDS} prediction tools that scan genome assemblies to identify \acp{ORF} that are subsequently filtered to identify putative \acp{CDS} \cite{dimonaco_no_2022}. Prodigal is a well-known \textit{ab initio} prokaryotic \ac{CDS} predictor that chewBBACA used up to version 3.3.0 \cite{hyatt_prodigal_2010, silva_chewbbaca_2018}. However, Prodigal has not been actively maintained, raising concerns about its long-term support. As part of the work developed in \textbf{\autoref{ch:paper1}} to improve chewBBACA, I substituted Prodigal by Pyrodigal \cite{larralde_pyrodigal_2022}. Pyrodigal provides Python bindings to Prodigal, improving the performance and offering greater control over the \ac{CDS} prediction process, while also ensuring results consistent with Prodigal's latest stable version\footnote{\url{https://github.com/hyattpd/Prodigal/releases/tag/v2.6.3}}. \textbf{\autoref{ch:paper1}} evaluated the performance of chewBBACA only at the module level. Benchmarking the key steps in each of chewBBACA's modules, especially if varying the number of \ac{CPU} cores used to better assess scalability, would have provided valuable information to understand which steps were most optimized and which still have room for improvement. Although the performance of the \ac{CDS} prediction step was not directly measured, it is safe to assume that Pyrodigal, which was optimized to reduce memory usage and runtime compared to Prodigal \cite{larralde_pyrodigal_2022}, contributed to the efficiency of chewBBACA 3 compared to its previous versions.

Although efficiency gains are definitely important, the true value of switching to Pyrodigal lies in having greater control over the \ac{CDS} prediction process. With Prodigal, \ac{CDS} prediction was only possible by calling Prodigal directly. In contrast, Pyrodigal provides Python bindings that enable finer control of the \ac{CDS} prediction parameters. This allowed to seamlessly integrate \ac{CDS} prediction into chewBBACA 3 and maintain results consistent with previous chewBBACA versions. In addition, this can be further explored to implement changes that can significantly improve the \ac{CDS} prediction process in future versions of chewBBACA 3. Possible improvements include selecting from multiple file formats for the output file containing the \ac{CDS} prediction results, such as GenBank and translated \acp{CDS}, which currently can only be written in FASTA format. Furthermore, the scores attributed to each \ac{CDS} can be accessed before writing the results, which, through a thorough validation process, may allow the definition of a score threshold that can be used to exclude spurious \acp{CDS} predicted by Pyrodigal, leading to improvements in allele calling and a reduction in the number of spurious loci included in the schema seeds created by the \textit{CreateSchema} module.

The prediction of spurious \acp{CDS} is the main disadvantage of using a \textit{ab initio} or model-based \ac{CDS} predictor. The predicted \acp{CDS} may not match any known gene, making validation difficult, or there may be a consistent deviation from reference alleles that have been experimentally validated, such as when the predicted \acp{CDS} are in-frame but shorter than the reference alleles or are out of frame compared to reference alleles \cite{dimonaco_no_2022}. This may contrast with alignment-based \ac{CDS} prediction, used in \ac{wg/cgMLST} platforms such as \ac{BIGSdb}, which may predict less spurious \acp{CDS} due to the use of more conservative parameters, but will have a limited capacity to identify more divergent alleles or \acp{CDS} corresponding to genes not in the reference database. The discrepancies between both \ac{CDS} prediction strategies hinder reproducibility. \ac{wg/cgMLST} schemas and results generated with chewBBACA 3 will not be directly comparable and may lead to different conclusions than the results generated on a platform that uses an alignment-based \ac{CDS} predictor. Adding alignment-based \ac{CDS} prediction to chewBBACA 3 can help complement Pyrodigal results, while also allowing users to define parameters that approximate those used by other \ac{wg/cgMLST} platforms, promoting interoperability and further adoption of chewBBACA 3. In fact, a combinatory approach is already used in some genome annotation pipelines, such as Prokka, Bakta, and PGAP, to improve \ac{CDS} prediction and annotation \cite{seemann_prokka_2014, schwengers_bakta_2021, li_refseq_2021}. Adopting a similar strategy in chewBBACA 3, especially if also defining a threshold based on the scores computed by Pyrodigal to filter out potential spurious \acp{CDS}, would definitely improve \ac{CDS} prediction and, consequently, the accuracy of the results. Validating the results produced by this strategy would require a thorough evaluation with reference datasets for both experimentally validated and spurious \acp{CDS}, which could prove to be a laborious process, but would certainly help establish a framework for more accurate \ac{CDS} prediction, especially in \ac{wg/cgMLST}.

\section{The assumption of dataset redundancy for large-scale wg/cgMLST and its potential limitations}

The average pairwise \ac{ANI} values of the species datasets used to evaluate chewBBACA 3 in \textbf{\autoref{ch:paper1}} reflect the composition of the public databases from which the genomes were sourced \cite{oleary_reference_2016, blackwell_exploring_2021}. A more uniform database composition does not necessarily correlate with global species diversity, as biases introduced by experimental protocols and uneven sampling, which favors specific bacterial strains, such as clinically relevant strains, may narrow our view on species diversity \cite{isaac_bias_2015, albright_trait_2023, ross_characterizing_2013, boers_understanding_2019}. Nonetheless, the species definitions in use correlate well with an intra-species ANI$\geq95\%$ \cite{rodriguez-r_ani_2023, jain_high_2018, castillo-ramirez_road_nodate, konstantinidis_sequence-discrete_2023, rossello-mora_past_2015, murray_re-evaluating_2021}, indicating that strains of the same species have very similar genomes and, as reported in \textbf{\autoref{ch:paper1}} when identifying the distinct set of \acp{CDS} for each dataset (see Table \ref{tab:ch2_tableS7}), may share a high proportion of identical \acp{CDS}. As demonstrated in \textbf{\autoref{ch:paper1}}, identifying the distinct set of \acp{CDS} shared by a group of strains by equality comparisons can significantly simplify allele identification and strain classification in \ac{wg/cgMLST} compared to approaches that rely more heavily on alignment.

The first published version of chewBBACA \cite{silva_chewbbaca_2018} already compared the \acp{CDS} predicted from the input genomes with the schema without using alignment, but would do this for every single genome, even if the same \ac{CDS} was shared by all genomes. To further optimize the exact matching process, I decided to implement a deduplication step after \ac{CDS} prediction. The deduplication identifies the set of distinct \acp{CDS}, returning a hash table with \ac{CDS} SHA-256 hashes as keys and the list of genomes containing each \ac{CDS} as values. The SHA-256 hashes are used as unique identifiers for exact matching between the predicted \acp{CDS} and the schema allele hashes, and are shorter than the actual \ac{CDS} sequences, which reduces memory usage compared to using nucleotide or protein sequences as keys. By associating the list of genomes with a particular \ac{CDS} to each distinct \ac{CDS} SHA-256 hash it is possible to identify and classify all genomes with a particular \ac{CDS}, avoiding redundant comparisons. An issue that I identified while testing \ac{CDS} deduplication with large datasets was that the lists of genome string identifiers associated to each key could contain many values and occupy a lot of memory, which meant that storing the hash table in memory could become impractical for systems with less than 16GB of \ac{RAM}. Substituting genome string identifiers with integer identifiers and using lists of integer identifiers as values reduced memory usage, but it was not enough to drop memory usage to a level at which most users could perform large-scale analyses. To further reduce memory usage, I implemented modified polyline encoding (inspired by the numcompress\footnote{\url{https://github.com/amit1rrr/numcompress}} package) to compress the lists of integers, producing strings that occupy less memory than the original lists, and allowing large-scale analyses with the memory available in most modern laptops, as reported in \textbf{\autoref{ch:paper1}}. Although implementing a solution for exact matching that integrates sequence deduplication, sequence hashing, and modified polyline encoding is more complex than simply comparing the \acp{CDS} sequences identified in each genome against a \ac{wg/cgMLST} schema, the gains in efficiency far outweigh the increase in complexity since it allows chewBBACA to be used at a scale that was not previously possible in a reasonable time frame.

A possible limitation of the strategy used for exact matching in chewBBACA 3 is that it assumes that the genomes in a dataset, especially as the size of the dataset increases, share a high proportion of identical \acp{CDS}. This was observed for the datasets used in \textbf{\autoref{ch:paper1}} and \textbf{\autoref{ch:paper3}}, and may be broadly generalizable when working with single-species datasets. However, at higher taxonomic ranks, such as at the genus level, sequence diversity may be considerably higher, increasing the size of the hash table and, consequently, memory usage. chewBBACA's performance at the genus level has not been tested in the results presented in this thesis, but there have been applications at that level, as evidenced by the \textit{Brucella}\footnote{\url{https://chewbbaca.online/species/11/schemas/1}} \cite{abdel-glil_core_2022} and \textit{Shewanella}\footnote{\url{https://chewbbaca.online/species/17/schemas/1}} schemas available on \ac{Chewie-NS}. Other \ac{wg/cgMLST} platforms, such as \ac{BIGSdb}\footnote{\url{https://pubmlst.org/}}, BIGSdb-Pasteur\footnote{\url{https://bigsdb.pasteur.fr/}} and EnteroBase\footnote{\url{https://enterobase.warwick.ac.uk/}} also store and manage schemas for genus-level typing, highlighting the interest and relevance of creating schemas for high-resolution typing at the genus level, especially when multiple species within a genus display pathogenic potential \cite{abdel-glil_core_2022, guglielmini_genus-wide_2019, pearce_proposed_2020, savin_genus-wide_2019}. Genus-level \ac{wg/cgMLST} schemas may be more broadly applicable than single-species schemas, providing high-resolution typing and a framework to study the inter-species loci diversity and gene flow, helping to elucidate key aspects in bacterial evolution and processes such as pathogenesis and virulence \cite{xie_inter-species_2024, dabernig-heinz_core_2024}. However, defining a set of target loci for genus-level applications may only be possible for species with sufficient genomic overlap. For genus that encompass very diverse species, it may only be possible to define a useful set of target loci for some of the species within the genus, if at all. Applications above the genus level are not envisioned, mainly because the degree of sequence divergence between taxa at higher taxonomic ranks may compromise the operational definition of loci used in \ac{wg/cgMLST}.

Another potential limitation is related to taxonomic underrepresentation in sequence databases, which is caused by both technical and non-technical factors. From a technical standpoint, sequencing and assembling the genome of certain bacterial species is more challenging due to limitations of sequencing technologies, difficulty or inability to culture, higher frequency of repetitive elements, and limitations of genome assembly software, among others. From a non-technical perspective, sequencing capacity can vary enormously between countries and tends to be used in studies or interventions deemed more relevant, such as sequencing bacterial species with a significant clinical or economic impact, leading to an incomplete representation of the diversity of bacterial species \cite{chorlton_ten_2024}. The application of methodologies that allow for less selective processing of biological samples, such as metagenomics, can help capture species diversity more accurately, promising a future in which reference databases are less biased. However, the impact of such approaches will not be immediate, nor are these approaches without challenges. For example, metagenomics may offer limited capacity to detect low-abundance bacterial species in samples with an overabundance of host \ac{DNA} \cite{mcardle_sensitivity_2020, gao_benchmarking_2025, govender_benchmarking_2022, constantinides_hostile_2023, billington_metagenomics_2022}. However, technical improvements are promoting the broader adoption of metagenomics in multiple areas, such as diagnostics, the detection of novel pathogens, and microbiome studies \cite{buddle_evaluating_2024}. This will inevitably reveal more of the diversity of bacterial species, leading to an increase in allele diversity and, consequently, an increase in memory usage for comprehensive \ac{wg/cgMLST}. The implementation of chewBBACA 3, presented in \textbf{\autoref{ch:paper1}}, demonstrates that it is possible to perform large-scale \ac{wg/cgMLST} with less than 8GB of \ac{RAM} in a reasonable time frame. Since most modern laptops include 8GB or more \ac{RAM}, using a laptop for large-scale \ac{wg/cgMLST} for any bacterial species is already a possibility, either by processing datasets representing the full diversity of the species or by dividing huge datasets into smaller datasets for species that are disproportionally represented in databases, such as \textit{Salmonella enterica}\footnote{\url{https://www.ncbi.nlm.nih.gov/datasets/genome/?taxon=28901}}. In addition, any concerns about increased memory usage may be minimized in the near future as the computational resources available to the general public become more and more powerful.

For large-scale \ac{wg/cgMLST} in systems with more modest computational resources, such as a laptop, storage capacity can also be a bottleneck. The input files in FASTA format and the intermediate data generated by chewBBACA 3 while processing large datasets (i.e., tens of thousands of genomes) may require a lot of storage space. The storage space required to process the genome collections of the most represented species in public databases such as the \ac{NCBI} may far exceed the storage capacity of most laptops. The storage and management of the huge amount of biological data that is generated has become a major challenge, representing significant costs and promoting research into more efficient data compression methods to reduce these costs \cite{chaudhari_biological_2024, betschart_benchmark_2025, nazari_lossless_2025}. Sequencing reads in FASTQ format and genome assemblies in FASTA format are usually compressed with \textit{gzip}\footnote{\url{https://www.gzip.org/}} to reduce the storage space used. Contrary to many bioinformatics software which accept compressed input files, chewBBACA 3 does not. Adding support for compressed input files and reducing the number and size of intermediate files created by chewBBACA 3 would allow more users to store and process larger datasets. General purpose compression algorithms are a convenient solution for the compression of biological data that is already included in many \ac{OS} or is easy to install. However, as more and more sequencing data is generated, the compression ratio provided by compression tools such as \textit{gzip} is insufficient, and it is necessary to use more efficient data compression methods specifically designed to compress biological data, such as AGC and MiniPhy \cite{deorowicz_agc_2023, brinda_efficient_2025}. To greatly reduce costs associated with data storage and enable efficient and granular searches over terabyte- or petabyte-scale datasets, institutions managing reference databases may have no choice but to switch and promote the adoption of much more efficient compression algorithms specifically designed for biological data.

\section{The importance and potential impact of combinatory approaches for the accuracy of wg/cgMLST}

The choice of methods for sequence comparison can have a great impact on the accuracy of bioinformatics analysis. Although alignment-based approaches remain the reference for sequence comparison, they can have some limitations \cite{zielezinski_alignment-free_2017}. For example, they tend to scale poorly for large datasets and provide less accurate results when comparing more divergent sequences with lower sequence identity due to the accumulation of point mutations and small indels or sequence rearrangements caused by recombination or \ac{MGEs} \cite{zielezinski_benchmarking_2019}. In the specific case of \ac{BLAST} \cite{altschul_basic_1990, camacho_blast_2009} applied to \ac{CDS} prediction and allele identification, local alignment extension can stop due to significant sequence divergence, resulting in a single or multiple local alignments that, while having a high score, do not meet the identity and coverage criteria enforced by \ac{wg/cgMLST} allele callers. This may result in the inability to identify loci that are, in fact, present in genomes.

In the case of chewBBACA 3, which only considers the highest scoring alignment reported by \ac{BLASTp}, the \ac{BSR} calculated from the best alignment may not reach the defined \ac{BSR} threshold simply because the reported alignment does not cover the majority of sequences that, apart from a single or a few dissimilar regions that interfere with alignment extension, are highly similar. In theory, combining multiple alignments may yield a \ac{BSR} that reaches the threshold, but there is no guarantee that \ac{BLAST} will report alignments between all similar regions. Moreover, \ac{BLAST} computes local alignments, and it seems reasonable that it does not provide any option to combine multiple local alignments to obtain a combined score. Initially, and perhaps naively, I thought it may be possible to implement a simple script to compute the combined score. But lack of knowledge about how the alignment raw score is computed, especially with compositional adjustment \cite{schaffer_improving_2001, yu_construction_2005}, made it seem like I was trying to probe a black box. Thus, it appeared reasonable to consider other options.

At first glance, one of the options could be to identify pairs of sequences for which \ac{BLASTp} reports multiple alignments that cover a significant fraction of the sequences and to compute global alignments to get a better estimate of global sequence similarity. Although a global alignment would cover the full length of the sequences being compared, two factors contributed to this option being ruled out nearly immediately. Firstly, global alignment yields different results than local alignment and I would have to use a different aligner than \ac{BLAST}. This would lead to completely different results, invalidating the \ac{BSR}-based approach used in chewBBACA, and could potentially diverge greatly from the results determined with \ac{BLAST}, which would warrant a complete overhaul of chewBBACA's algorithm, both conceptually and algorithmically. Secondly, global alignment is computationally expensive and is not adequate to align large collections of potentially divergent sequences. Since I intended to implement a new version of chewBBACA tailored for scalable and efficient analysis of large genome collections, I decided instead to complement the alignment-based strategy used by chewBBACA with alignment-free methods, more specifically \textit{k}-mer-based methods \cite{swain_interpreting_2022, zielezinski_alignment-free_2017}. The concept of \textit{k}-mers, although seemingly simple, offers extreme versatility by simply varying the value of \textit{k}, which defines the size of the \textit{k}-mers, and choosing a sampling method to select subsets of \textit{k}-mers from a sequence \cite{shaw_theory_2022}. Research into the optimization of \textit{k}-mer-based methods has produced concepts such as spaced seeds \cite{brinda_spaced_2015, hantze_effects_2023}, minimizers \cite{schleimer_winnowing_nodate, ndiaye_when_2024, belbasi_minimizer_2022, roberts_reducing_2004, marcais_improving_2017, zheng_improved_2020}, syncmers \cite{edgar_syncmers_2021, dutta_parameterized_2022}, and more recently strobemers \cite{sahlin_effective_2021, karami_designing_2024, moeckel_survey_2024}. These concepts are applicable to virtually any step of the bioinformatics workflow, contributing to the efficiency and accuracy of methods for read mapping \cite{alser_technology_2021, vasimuddin_efficient_2019, li_minimap2_2018, sahlin_survey_2023}, sequence alignment \cite{camacho_blast_2009, gaston_x-mapper_2025, clausen_rapid_2018}, taxonomic classification \cite{menzel_fast_2016, song_centrifuger_2024, kim_centrifuge_2016, portik_evaluation_2022, lu_metagenome_2022, wood_improved_2019}, and metagenomic binning \cite{han_benchmarking_2025, wu_maxbin_2016, pan_semibin2_2023}, among others \cite{moeckel_survey_2024}. Unsurprisingly, there are also examples of the application of alignment-free methods in \ac{wg/cgMLST}, such as using KMA \cite{clausen_rapid_2018} to perform \ac{cgMLST} directly from sequencing reads\footnote{\url{https://github.com/BCCDC-PHL/kma-cgmlst}} and the EToKi\footnote{\url{https://github.com/zheminzhou/EToKi}} suite of methods used by Enterobase \cite{zhou_enterobase_2020, zhou_accurate_2020}. Alignment-free methods have become undeniably ubiquitous in bioinformatics, either because they allow for more efficient and scalable analysis or because they overcome some of the limitations of alignment-based methods \cite{borozan_integrating_2015, zielezinski_alignment-free_2017}.

I chose minimizer-based clustering to complement the alignment-based strategy used by chewBBACA because minimizers are simple to implement, fast to determine, and enabled me to implement a clustering method that was accurate enough for the application I had in mind. Performing intra-cluster alignment to classify the \acp{CDS} is much faster than all-vs-all or pairwise alignment between the \acp{CDS} identified in all inputs. Minimizer-based clustering also enables the identification of highly similar sequences with differences concentrated in a single or a few sequence regions because it counts the number of shared minimizers along the complete sequences, whereas \ac{BLAST} may only report smaller local alignments, making it difficult to determine global sequence identity. The strength of the strategy lies then in combining alignment-based and alignment-free methods to improve speed and accuracy, an approach that has been increasingly used in bioinformatics. Although minimizer-based clustering enabled considerably faster and slightly more accurate \ac{wg/cgMLST} with chewBBACA 3, as reported in \textbf{\autoref{ch:paper1}}, it is not sufficiently sensitive to identify some of the most divergent alleles within the $0.6\geq BSR > 0.7$ interval. Due to this limitation, the identification of the most divergent alleles, performed by the default execution mode, mode 4, uses only \ac{BLASTp}, which can increase the runtime with larger and more diverse datasets. An alternative approach would be to accept a slight reduction in sensitivity and use minimizer-based clustering anyway, since the number of more divergent alleles not identified would probably be small, as evidenced by the small differences between chewBBACA execution modes 3 and 4 (see Figures \ref{fig:chap2_figureS15} and \ref{fig:chap2_figureS16}). However, since I did not test this hypothesis with datasets of other species, I decided to opt for the more conservative approach. The limitation of minimizer-based clustering in the identification of divergent alleles may be surpassed by the implementation of spaced-minimizer- or strobemer-based clustering. Spaced-minimizers and strobemers allow for sequence variation between matching regions, which can better accommodate the differences identified in more divergent alleles \cite{sahlin_effective_2021, karami_designing_2024}. If successful, this approach could further reduce chewBBACA's runtime and improve accuracy.

To further increase the accuracy of schema creation and allele calling, creating graphs representing intra- and inter-cluster connections between similar \acp{CDS} could enable the identification of connected components matching groups of similar \acp{CDS}. These connected components would facilitate the identification of alleles belonging to the same locus, including divergent ones not directly linked but sharing a BSR $\geq0.6$ with other alleles in the same connected component, and improve the detection of paralogous loci and pseudogenes based on sequence similarity and genomic context analysis. Furthermore, including the frequency of each \ac{CDS} in the input genomes as a node attribute could help identify and select the most frequent \acp{CDS} as loci representatives during schema creation to better capture loci diversity. This contrasts with the current strategy used in chewBBACA 3, which selects a single representative allele, limiting the identification of new alleles based on a maximum sequence divergence from the single representative allele defined by the \ac{BSR} and the size threshold parameters. The resolution of the \ac{wg/cgMLST} analysis performed by chewBBACA 3 can also be expanded by performing \ac{SNA} based on the \ac{CDS} coordinate data determined by chewBBACA 3, a resource that has remained largely unused. Loci adjacency can be represented as a graph, allowing to identify paths corresponding to groups of loci that are frequently conserved in order and orientation, called \ac{SBs}. Variations in the composition of similar \ac{SBs} can help identify structural changes caused by events such as \ac{HR} and \ac{HGT} \cite{enav_strain_2025, moore_kmeraperture_2024}. Comparison of strains based on the composition and presence-absence analysis of \ac{SBs} can potentially improve the estimation of relatedness in \ac{wg/cgMLST} and help identify groups of loci correlated with relevant phenotypes associated with increased virulence and \ac{AMR}. The results of the \ac{SNA} could be included in the report generated by the \textit{AlleleCallEvaluator} module, enabling users to explore the composition of identified \ac{SBs} and compare bacterial strains based on synteny.

\section{Providing functionalities for comprehensive and user-centered wg/cgMLST data analysis is essential to fully leverage the potential of wg/cgMLST}

Improving the accuracy of \ac{wg/cgMLST} schema creation and allele calling is crucial to provide reliable results. However, assuming that minimum requirements for accuracy have been met, end-users are usually more interested in downstream analyses, such as the computation of distance matrices and \ac{MST}s to identify closely related strains, and the attribution of \ac{ST}s based on pre-defined nomenclatures. Those analyses can provide valuable information in surveillance and outbreak detection settings, and there are several tools and platforms that enable them \cite{nascimento_phyloviz_2017, ribeiro-goncalves_phyloviz_2016, zhou_grapetree_2018, mixao_reportree_2023, de_ruvo_spread_2024}. However, \ac{wg/cgMLST} schemas and results hold the potential for much more detailed analyses that go beyond traditional applications in surveillance and outbreak detection. For example, \ac{wg/cgMLST} was used to evaluate the genomic diversity of an emerging human pathogen and characterize candidate antigens for vaccine development \cite{isidro_virulence_2020, klimka_epitope-specific_2021, machimbirike_comparative_2021}.

I developed chewBBACA's \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules to allow users to explore \ac{wg/cgMLST} schemas and results intuitively and comprehensively. The information and functionalities provided in the reports generated by both modules allow users to evaluate loci diversity and assess strain similarity in great detail at the dataset level, and up to the species level if analyzing \ac{wg/cgMLST} schemas and datasets that capture the full diversity of the species. In addition, reports are generated locally to enable more scalable and shareable analyzes compared to centralized platforms with limited resources and to avoid issues related to data privacy concerns. Although the reports already include functionalities for a detailed analysis of \ac{wg/cgMLST} results, I think they can be further expanded to improve their applicability.

While the \textit{SchemaEvaluator} module has been used to evaluate schemas not generated with chewBBACA, such as schemas available in \ac{BIGSdb} and Enterobase \cite{jolley_bigsdb_2010, zhou_enterobase_2020}, this option was not extensively tested and it remains unclear if the module can be used to evaluate any schema regardless of provenance. The \textit{AlleleCallEvaluator} module only accepts results generated with chewBBACA, limiting its applicability. Changing both modules to make them compatible with the most widely used schema and allele call results data formats would definitely benefit most users. Additionally, the allele call results can be utilized more effectively by developing new functionalities that enable the computation of loci and sample sets based on loci presence thresholds directly in the report and identify lists of potentially spurious loci and low-quality genomes based on the percentage of missing data.

The special classifications assigned by chewBBACA 3 (see Figures \ref{fig:chap2_figureS1}-\ref{fig:chap2_figureS4}) are usually ignored or discarded while analyzing allele calling results. This is a reasonable approach because the ambiguous nature of these classifications does not allow to infer loci presence confidently. Inferring loci presence based on these classifications can reduce the accuracy of common downstream analysis, such as distance estimation and core genome computation. However, simply treating these classifications as missing data also undervalues their potential. \acp{CDS} assigned special classifications may correspond to valid alleles of biological relevance, such as more divergent alleles, alleles for new loci that are not in the schema and alleles of paralogous genes. Special classifications are also assigned to spurious \acp{CDS} more frequently, with higher frequencies of special classifications being a sign of lower-quality data. Including a more detailed analysis of the \acp{CDS} assigned special classifications in chewBBACA's reports, including their genomic context, could provide valuable information to identify new alleles, paralogous genes, pseudogenes, and spurious \acp{CDS} resulting from misassembly and frameshift mutations.

There are also features that, while perhaps more challenging to implement, would extend the report's functionalities to the point where most users would not need to resort to other software for further insight. For example, determining the variable positions or \acp{SNP} per locus would tap into the resolution level of \ac{SNP}-based approaches. New features for sample clustering and \ac{ST} assignment, which are commonly performed by users using other tools, based on a \textit{ad hoc} or pre-defined nomenclature have been requested by chewBBACA users, and could probably be incorporated into chewBBACA by integrating the results generated by ReporTree \cite{mixao_reportree_2023}. The report generated by the \textit{SchemaEvaluator} module provides detailed information about the number of alleles and allele size per locus, as well as a \ac{MSA} component to compare translated alleles and identify sequence differences, but it does not provide a more general measure of similarity at the intra- and inter-locus level. To that end, representations of intra- and inter-locus similarity based on the \ac{BSR} could help users quickly identify loci with greater variability and groups of similar loci. A dedicated page for each sample analyzed would be a valuable addition to the report generated by the \textit{AlleleCallEvaluator} module. Each sample page would include summary classification statistics and a circular genome viewer, such as CGView.js\footnote{\url{https://js.cgview.ca/index.html}} \cite{stothard_visualizing_2019}, which supports the representation of annotated loci features, providing an easy way to explore genome structure, identify loci of interest, and explore their genomic context.

chewBBACA 3, as many other bioinformatics tools, is a \ac{CLI} tool compatible only with Unix-like operating systems such as Linux. While running chewBBACA's modules is not very complex, it still requires users to be familiar with the command line. The lack of a \ac{GUI} is perhaps the greatest hurdle to the wide adoption of bioinformatics tools by users without bioinformatics training. Implementing a \ac{GUI} and making chewBBACA compatible with Windows would definitely encourage more researchers to perform \ac{wg/cgMLST} and explore the functionalities provided by chewBBACA.

\ac{Chewie-NS}, presented in \textbf{\autoref{ch:paper2}}, was created to be less restrictive in terms of data sharing policies compared to centralized platforms such as \ac{BIGSdb}. In that regard, I think \ac{Chewie-NS} succeeds in providing uncomplicated access to \ac{wg/cgMLST} schemas that can be used for local and private analysis based on a common allelic nomenclature. However, \ac{Chewie-NS} lacks functionalities for a comprehensive analysis of schema structure and loci diversity compared to other well-known \ac{wg/cgMLST} platforms. Although the schema and loci pages in \ac{Chewie-NS} served as templates for the initial versions of the reports created by chewBBACA's \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules, the pages in \ac{Chewie-NS} have become outdated compared to chewBBACA's reports. Updating the schema and loci pages in \ac{Chewie-NS} to at least match the functionalities provided by the chewBBACA reports would benefit the users browsing the schemas deposited in \ac{Chewie-NS}, essentially providing a catalog of the loci identified in bacterial species and their diversity. The decentralization of \ac{wg/cgMLST} analysis is the main strength of \ac{Chewie-NS}. However, it can also be a major disadvantage if users do not have the expertise to perform local \ac{wg/cgMLST} and request or send data through \ac{Chewie-NS}' \ac{API}. While it is not the objective of \ac{Chewie-NS} to centralize \ac{wg/cgMLST} analysis, I think implementing pages for the submission of data and the analysis of results would allow more users, especially those without bioinformatics training, to use the schemas deposited in \ac{Chewie-NS}. These operations would be optional, with a fully decentralized workflow still possible. Furthermore, to promote interoperability and the applicability of the allelic nomenclatures managed by \ac{Chewie-NS}, it would be beneficial to store allelic profiles and allow users to compare their results against stored profiles based on \ac{Chewie-NS}' nomenclature and the nomenclatures implemented by other \ac{wg/cgMLST} platforms. Since data submission would be optional, the database of allelic profiles would be populated by periodically downloading high-quality genomes from public databases such as the \ac{NCBI} and performing allele calling in \ac{Chewie-NS}, providing a strong basis for users to contextualize and compare their strains.

\section{The unrealized potential of wgMLST}

I have frequently referred to \ac{wg/cgMLST}, but in reality the majority of analyses is performed at the \ac{cgMLST} level and \ac{wg/cgMLST} platforms store mostly \ac{cgMLST} schemas\footnote{The \ac{cgMLST} schemas used with the commercial software Ridom SeqSphere+ are available at \url{https://www.cgmlst.org/ncs} and an instance of the \ac{BIGSdb} platform managed by the Institut Pasteur that stores \ac{cgMLST} schemas for multiple \textit{taxa} is available at \url{https://bigsdb.pasteur.fr/}} \cite{jolley_open-access_2018, dyer_enterobase_2025}. Working at the \ac{cgMLST} level provides robust results for most applications, such as surveillance, and avoids having to deal with the issue of missing data when considering a greater number of loci, such as when working at the \ac{wgMLST} level \cite{leeper_evaluation_2023, uelze_typing_2020}. When greater resolution is needed, such as in outbreak investigation, \ac{cgMLST} results can be complemented with other methods, such as \ac{SNP}-based approaches using a closely related reference genome or \ac{wgMLST} \cite{mixao_multi-country_2025, leeper_validation_2025, leeper_evaluation_2023}. The process of creating a \ac{cgMLST} schema is relatively simple, especially compared to the creation of a robust \ac{wgMLST} schema, such as the one created for \textit{S. pyogenes} in \textbf{\autoref{ch:paper3}}. A simple workflow for \ac{cgMLST} schema creation that takes advantage of chewBBACA's functionalities consists in selecting a dataset of high-quality closed genomes for a bacterial species to create a schema with the \textit{CreateSchema} module, followed by allele calling with a more diverse dataset and core-genome determination with the \textit{ExtractCgMLST} module. This process has been applied many times to create \ac{cgMLST} schemas for high-resolution typing \cite{tourasse_core_2023, de_sales_core_2020, hershko_construction_2024, kozak_core_2025, crestani_microevolution_2025, carneiro_genome-scale_2023, thorell_helicobacter_2023, feng_population_2024, jansen_van_rensburg_development_2024}. Identifying and removing spurious and paralogous loci may further increase the quality of \ac{cgMLST} schemas, but these issues tend to be few if the genomes used for schema creation are carefully selected. Scaling to \ac{wgMLST} is more complex, as it incorporates the accessory genome, which can be highly variable, in part due to the effect of recombination and \ac{MGEs} \cite{medini_microbial_2005, tettelin_genome_2005, costa_first_2020}.

The variability of the accessory genome represents a challenge in the creation of \ac{wgMLST} schemas. Incorporating the accessory genome leads to an increase in the number of spurious loci in \ac{wgMLST} schemas due to a higher frequency of pseudogenes, gene fusions, paralogous genes, repetitive elements, and genes whose allele diversity can only be fully captured by defining custom parameters, rather than using schema-wide parameters. This diversity of features blurs the limits of loci and allele definitions. To make matters worse, features such as repetitive elements complicate the genome assembly process, potentially leading to a higher number of sequence assembly errors, which introduces uncertainty about the validity of the identified features \cite{merda_unraveling_2024, wick_unicycler_2017}. These issues were encountered when creating the \ac{wgMLST} schema for \textit{S. pyogenes}, as described in \textbf{\autoref{ch:paper3}}. The initial \ac{wgMLST} schema, containing 3,318 distinct loci, identified from a dataset of high-quality 208 complete genomes, was curated to identify and correct issues, resulting in a final schema with 3,044 loci. The reduction in the number of loci was not due to simply excluding loci from the schema. It was the result of a laborious curation process guided by an expert in \textit{S. pyogenes} to identify spurious schema loci corresponding to pseudogenes, gene fusions, and paralogous loci based on the functional annotation of all loci and on the inspection of the sequences and genomic context of hundreds of schema loci. The final curation process consisted of the substitution of schema loci that matched pseudogenes and gene fusions by valid alleles, merging schema loci corresponding to the same gene, and removing schema loci only when it was not possible to identify valid alternative alleles. Removing groups of paralogous loci identified by chewBBACA and loci mostly assigned special classifications is a simpler alternative to the laborious curation process used to create the \textit{S. pyogenes} \ac{wgMLST} schema, but it potentially removes a lot of loci that can be corrected through a more careful curation process. A detailed curation process maximizes the quality and loci diversity accurately captured by a \ac{wgMLST} schema, but the requirements of such a process are a great limitation to developing comprehensive and reliable \ac{wgMLST} schemas. Moreover, some studies suggest that \ac{wgMLST} does not constitute a very significant improvement in terms of discriminatory power over \ac{cgMLST} \cite{uelze_typing_2020, henri_assessment_2017, pearce_comparative_2018, vincent_comparison_2018, joseph_evaluation_2023, blanc_comparison_2020, baktash_comparison_nodate, king_comparison_2024}. So why even bother creating \ac{wgMLST} schemas?

Regarding the requirements for creating \ac{wgMLST} schemas, I consider that we are simply missing the tools to create high-quality \ac{wgMLST} schemas. In fact, I started a project called Schema Refinery\footnote{\url{https://github.com/B-UMMI/Schema_Refinery}} with the objective of creating a set of tools to help users perform common steps of \ac{wg/cgMLST} schema creation. Initially, I included Python scripts to download and select genome assemblies for schema creation and allele calling, annotate schema loci based on multiple sources, and perform basic operations to refine schemas such as merging, splitting, and excluding loci. Schema Refinery has been under active development, although the bulk of the development has been passed to other members of the lab, while I participate in the discussions about implementation design, test the functionalities, and occasionally contribute with code changes. Since the old adage of \ac{GIGO} is also valid for \ac{wg/cgMLST}, Schema Refinery enables the download and selection of high-quality genomes for schema creation and allele calling, which will greatly reduce the number of issues related to spurious loci identified in low-quality genome assemblies. Furthermore, Schema Refinery includes a module to refine and expand \ac{wg/cgMLST} schemas. This module allows users to identify and resolve issues such as those identified during the creation of the \textit{S. pyogenes} \ac{wgMLST} schema automatically, and will provide the option to identify new loci from chewBBACA's allele calling results to add to the schemas. The possibility of automatically refining schemas will greatly simplify the creation of high-quality \ac{wgMLST} schemas, and the option of adding new loci has the potential to be a future-proof solution to update schemas as we reveal more of the diversity of bacterial species.

Concerning the observation that \ac{wgMLST} may not constitute a considerable improvement over \ac{cgMLST}, I think such a conclusion can only be drawn from experiments whose objectives do not fully explore the potential of \ac{wgMLST}. Firstly, \ac{wgMLST} schemas can contain a lot more loci than \ac{cgMLST} schemas, often more than double the loci, capturing much more of the diversity of bacterial species. \ac{wgMLST} and \ac{cgMLST} results can be strongly correlated, leading to similar observations and conclusions when comparing, for example, tree topologies. However, it is important to compare the resolution provided by both approaches in terms of the number of loci being compared and the differences identified between bacterial strains. A \ac{cgMLST} schema that contains half or less than the average number of loci in the genomes of a bacterial species cannot possibly provide the same resolution as a \ac{wgMLST} schema that includes nearly every locus ever identified in the genomes of the same species. Thus, an apparent equivalence of both approaches is only true when the study objectives limit the analysis to a lower resolution level than what \ac{wgMLST} can provide. It is important to establish a clear distinction between discriminatory power and resolution. For a specific dataset, both approaches can identify the same clusters of closely related strains, providing equivalent discriminatory power for clustering. However, this is highly dependent on the dataset being analyzed and on the parameters used for cluster definition. The higher resolution provided by \ac{wgMLST} allows to compare and explore the diversity of more loci than \ac{cgMLST} and offers greater discriminatory power when differences are more concentrated on the accessory genome. Multiple studies have compared \ac{cgMLST} and \ac{wgMLST}, demonstrating that \ac{wgMLST} provides further resolution when compared with the \ac{cgMLST} approaches routinely used for surveillance and outbreak investigation \cite{mixao_multi-country_2025, baktash_comparison_nodate, joseph_evaluation_2023, leeper_validation_2025}. In addition, a detailed analysis of the loci and population diversity of a species, such as what is performed in pangenome analysis, is only possible at the \ac{wgMLST} level, as restricting the analysis to the \ac{cgMLST} level would under-evaluate the genetic diversity of the species \cite{tettelin_genome_2005}. In fact, a \ac{wgMLST} schema that captures a species diversity could also be called a \ac{pgMLST} schema, and could, in theory, allow for pangenome analysis with chewBBACA 3 by analyzing a dataset representative of the diversity of the species with the \textit{AlleleCall} module and generating reports with the the \textit{SchemaEvaluator} and \textit{AlleleCallEvaluator} modules. It is also important to note that genes determinant for relevant phenotypic characteristics, such as virulence and antibiotic resistance, are often part of the accessory genome and may not be identified by \ac{cgMLST} \cite{holt_genomic_2015, jackson_influence_2011, darmancier_are_2022}. The increased resolution of \ac{wgMLST} can be especially valuable in the context of surveillance and outbreak investigation, as demonstrated by the identification of the \textit{S. pyogenes} M1 lineages in \textbf{\autoref{ch:paper3}}. In that case, the analysis was performed by focusing on the core genes shared by the lineages' strains, but since the schema used was a \ac{wgMLST} schema it allowed to tune up the core-genome based on the dataset, which would not be possible with a more strict \ac{cgMLST} schema. The strategy of using a \ac{wgMLST} schema to scale the size of the core-genome based on the dataset under analysis can enrich surveillance and outbreak detection analyzes, but it has not been adopted as routine. Furthermore, the increased resolution resulting from the transition to the \ac{wgMLST} schemas would invalidate the distance thresholds for outbreak definition currently used. Thus, transitioning to routine \ac{wgMLST} would require a comprehensive analysis to identify congruence points and establish equivalent distance values, ideally by defining flexible thresholds that better accommodate the differences between the surveillance systems implemented by public health institutions\cite{mixao_multi-country_2025}.

Although using \ac{wgMLST} schemas to dynamically define the core-genome for each dataset represents a step forward from using stricter \ac{cgMLST} schemas, it still does not use all loci in \ac{wgMLST} schemas, which can identify additional loci exclusive to strain subsets in the dataset under analysis. Fully transitioning to \ac{wgMLST} is complex, in part due to uncertainty surrounding missing data. Missing data corresponds to loci that are not identified in the genomes, classified as absent (e.g. the \textit{LNF} classification assigned by chewBBACA), or to loci for which the validity of the match found is somewhat ambiguous (e.g., the match is considerably shorter or longer than the matched locus, corresponding to the \textit{ASM} and \textit{ALM} classifications assigned by chewBBACA, respectively). Most of \ac{WGS} data are available as sequencing reads in FASTQ format or, if assembled, as draft genome assemblies in FASTA format. The impossibility of having complete genomes introduces uncertainty about loci presence, as we cannot always distinguish between loci that are truly absent or appear absent due to issues introduced by sequencing and assembly errors. The loci presence threshold used to determine the core genome from \ac{wg/cgMLST} data is usually set to 99\% or 95\%, instead of 100\%, precisely to accommodate issues such as these that cause a drop in the frequency of highly frequent loci. The transition to \ac{wgMLST} is also hampered by the methods and parameters used for loci identification. Defining simple parameters for loci identification, such as 80\% sequence identify and coverage, or a \ac{BSR} $\geq0.6$ as used by chewBBACA 3, represents an oversimplification of locus diversity. A single or a few parameter values do not allow for perfect loci identification. Perfect loci identification, if possible at all, may require the definition of finely tuned parameter values for each locus. Some \ac{wg/cgMLST} platforms, such as \ac{BIGSdb} allow to define locus-specific parameter values. However, we cannot determine the optimal set of parameters for every locus. Thus, I think that, in the meantime, we will have to continue to adjust our generalizations. Uncertainty and doubt remain, both important in science.

\section{Perspectives on the future of wg/cgMLST}

Bioinformatics, as an interdisciplinary field, has greatly benefited from the application of concepts and methods derived from other scientific fields. The achievement of technological milestones and growing interest are propelling bioinformatics into a flourishing age and to the center stage of scientific research. The performance of computing hardware has been steadily improving and has reached a point where it is possible to generate, store, and analyze huge amounts of biological data. More widespread access to powerful computational resources allows more researchers to apply existing or newly developed methods to tackle challenging problems that were once off limits due to technological and technical constraints. Several public health emergencies, such as the COVID-19 pandemic, caused by the SARS-CoV-2 virus, and mortality caused by major bacterial pathogens, especially in low- and middle-income countries, have also raised the public and governmental organizations' awareness of the importance of research and preventive measures \cite{ikuta_global_2022}.

The boom in \ac{GPU} computing, in conjunction with the growing interest and advances in \ac{AI}, will definitely continue to accelerate biomedical research and expand the realm of research possibilities. A good example of a recent and monumental breakthrough due to the application of \ac{GPU} computing and \ac{AI} is AlphaFold \cite{jumper_highly_2021, abramson_accurate_2024}, which has greatly improved our ability to accurately predict protein structures. The true impact of AlphaFold will probably only be fully realized in the coming years. \ac{AI} has already been applied to prokaryotic gene prediction \cite{sommer_balrog_2021}, a crucial aspect for accurate \ac{wg/cgMLST}. A more accurate prediction of protein structure and function can also help validate gene prediction data, contributing to improvements in gene prediction and \ac{wg/cgMLST} schema refinement.

When talking about the future of \ac{wg/cgMLST}, one cannot simply forget to mention the relatively recent improvements to \ac{DNA} sequencing technologies that have enabled \ac{WGS} and consequently \ac{wg/cgMLST}. Further improvements to these technologies are expected. Any improvements, especially in the accuracy of long-read sequencing, will increase the contiguity of assembled genomes and perhaps even make accurate \ac{wg/cgMLST} from sequencing reads a reality. More contiguous genome assemblies should also lead to more accurate gene prediction. The quality of a genome assembly is influenced by multiple factors, including the methods used for sample preparation, the sequencing technology used, and the tools used to process the sequencing data and generate a genome assembly. Each step of the process and the combination of methods used throughout the process can introduce biases that negatively influence the end result \cite{merda_unraveling_2024}. These issues contribute differently to the degree of fragmentation of genome assemblies, often leading to a cumulative effect that affects a greater number of loci as more genomes are analyzed. In my view, improvements to \ac{DNA} sequencing technologies and the standardization of other steps can lead to much more than a slight increase in the accuracy of \ac{wg/cgMLST}. It can greatly reduce the number of absent or fragmented genes, facilitating the transition to \ac{wgMLST}, and possibly revealing that we need to rethink the definitions of the core and accessory genome for many bacterial species.

I also consider that we are nearing a crossroads where we will have to rethink, or at least adapt, the methods used for \ac{wg/cgMLST}. The Web platforms and other software for \ac{wg/cgMLST} are responsible and are also a consequence of the success of \ac{wg/cgMLST}. \ac{wg/cgMLST} allows for a comprehensive analysis of loci diversity and high-resolution typing in surveillance and outbreak scenarios, but even at its highest potential, it may lack some of the features of \ac{SNP}-based and \textit{k}-mer-based methods. In fact, \ac{wg/cgMLST} results are often complemented by \ac{SNP} analysis for finer resolution, such as in outbreak detection. The integration of \textit{k}-mer-based methods can also increase the accuracy and speed of the analyzes, as achieved with chewBBACA 3 in \textbf{\autoref{ch:paper1}}. I think these different approaches will be increasingly combined to take advantage of the strengths of each one, with discussions such as allele-based vs. \ac{SNP}-based becoming increasingly less relevant and priorities shifting towards the implementation of software that performs comprehensive and multifaceted analyzes. In addition, I think graph-based methods can emerge as an alternative that surpasses the accuracy of other approaches. Improvements in computing hardware and graph algorithms, as well as the availability of a large number of sequenced genomes, have made it possible to index and examine the variation of larger genome collections \cite{holley_bifrost_2020, harling-lee_graph-based_2022, noll_pangraph_2023}. Pangenome graphs have been used to index and examine the variability of bacterial genomes, allowing the identification of population-level nucleotide and structural polymorphisms. Using graph data structures to index the variability of coding and non-coding regions of large collections of bacterial genomes may provide a framework that achieves results more accurate than the combination of \ac{wg/cgMLST} and \ac{SNP}-based approaches by allowing the analysis of non-coding regions, which cannot be done with \ac{wg/cgMLST}, and surpassing the reference bias issue of \ac{SNP}-based methods. All while also providing information about genome structure, which would allow to obtain information about genomic context through synteny analysis. Notwithstanding the potential of graph-based methods, the computational requirements for the construction of graphs that encompass the diversity of thousands of bacterial strains may be prohibitive for most users, preventing the applicability of graph-based methods compared to \ac{wg/cgMLST}.

The comparability of the \ac{wg/cgMLST} results at the national and international levels is essential for effective surveillance and outbreak detection. During the COVID-19 pandemic, the scientific community quickly adapted and established data analysis and sharing standards to promote easy sharing and comparability of results to track SARS-CoV-2 variants \cite{maxwell_fair_2023, griffiths_future-proofing_2022}. In contrast, the stage of implementation of \ac{WGS}-based surveillance systems for bacterial pathogens, such as for \ac{FWD} pathogens, can differ significantly between countries \cite{mixao_multi-country_2025}. The disparities between the surveillance systems implemented by different countries hinder the comparability of the results at the intersectoral and international levels, which affects outbreak detection and investigation, especially for multi-country outbreaks. A detailed congruence analysis of the results generated by eleven European institutes revealed that, while there may be general concordance between the results generated by similar surveillance systems, the results are not directly comparable \cite{mixao_multi-country_2025}. Different surveillance systems provide different levels of resolution, and a congruence analysis is necessary to establish threshold equivalence between systems. Since the implementation of a surveillance system represents a significant investment, it is highly unlikely that any country will switch to a different approach in the short term, even if that would promote interoperability and provide more accurate results. A more immediate solution would be to determine congruence points between all the different systems and using flexible thresholds to accommodate for incompatibilities arising due to the use of single value thresholds. However, this approach would require a significant collaboration effort between institutions of many different countries and the validity of the congruence system would have to be continuously assessed. In the long term, the systems used by different countries may converge toward methods that are provably better, promoting interoperability and approximating a global One Health surveillance system. In the meantime, countries will likely continue sharing data on an \textit{ad hoc} basis or through centralized systems such as \ac{EFSA}'s One Health WGS system when a more concerted action is necessary to track and resolve multi-country outbreaks.

It is clear that the potential of the \ac{wg/cgMLST} approach has not yet been fully realized, with room for improvement both conceptually and technically. The ongoing technological revolution also expands the realm of possibilities, encouraging researchers to apply methodologies that were once unfeasible from a technological standpoint or come up with revolutionary strategies arising from a shift in perspective. The adoption and evolution of \ac{wg/cgMLST} is influenced by multiple factors that go beyond scientific objectiveness, making it difficult to predict the magnitude of its future role. However, future bacterial surveillance systems and population diversity studies, even if methodologically distinct from \ac{wg/cgMLST}, will surely have to incorporate the advantageous properties of \ac{wg/cgMLST} that distinguish it from other approaches currently used. The current scientific revolution will surely continue to provide a multitude of possibilities and opportunities that will approximate and blur the boundaries between scientific fields. The uncertainty of how it will all unfold is undoubtedly exciting.
